"use strict";(self.webpackChunkdoc=self.webpackChunkdoc||[]).push([[4864],{8422:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>u,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var t=n(4848),a=n(8453);const i={},o=void 0,r={id:"video_transcription",title:"video_transcription",description:"cette vid\xe9o va vous donner tout ce dont vous avez besoin pour ne rien savoir de l'intelligence artificielle et des grands mod\xe8les de langage et avoir une base solide sur le fonctionnement de ces technologies r\xe9volutionnaires au cours de l'ann\xe9e \xe9coul\xe9e. L' intelligence artificielle a compl\xe8tement chang\xe9 le monde avec des produits comme chat PT potentiellement annexant chaque industrie et la fa\xe7on dont les gens interagissent avec la technologie en g\xe9n\xe9ral et dans cette vid\xe9o, je me concentrerai sur la fa\xe7on dont ils fonctionnent, les applications d'it\xe9rations et bien plus encore et cette vid\xe9o a \xe9t\xe9 cr\xe9\xe9e en collaboration avec un programme incroyable appel\xe9 AI camp dans lequel les lyc\xe9ens apprennent tout sur l' intelligence artificielle et j'en parlerai davantage plus tard dans la vid\xe9o, commen\xe7ons par qu'est-ce qu'un LLM, est-il diff\xe9rent de l'IA et en quoi le chat GPT est-il li\xe9 \xe0 tout cela ? Les LLM repr\xe9sentent de grands mod\xe8les de langage qui  est un type de r\xe9seau neuronal form\xe9 sur d'\xe9normes quantit\xe9s de donn\xe9es textuelles. Il est g\xe9n\xe9ralement form\xe9 sur des donn\xe9es qui peuvent \xeatre trouv\xe9es en ligne, du web scraping aux livres en passant par les transcriptions. Tout ce qui est bas\xe9 sur du texte peut \xeatre form\xe9 dans un grand mod\xe8le de langage et prendre du recul.  qu'est-ce qu'un r\xe9seau de neurones, un r\xe9seau de neurones est essentiellement une s\xe9rie d'algorithmes qui tentent de reconna\xeetre des mod\xe8les dans les donn\xe9es et ce qu'ils essaient en r\xe9alit\xe9 de simuler le fonctionnement du cerveau humain et les LLM sont un type sp\xe9cifique de r\xe9seau de neurones qui se concentre sur la compr\xe9hension.  le langage naturel et, comme mentionn\xe9, les llms apprennent en lisant des tonnes de livres, d'articles, de textes sur Internet et il n'y a vraiment aucune limitation. En quoi les llms diff\xe8rent-ils de la programmation traditionnelle? La programmation traditionnelle est bas\xe9e sur des instructions, ce qui signifie que si x, alors pourquoi vous dites explicitement \xe0 l' ordinateur  que faire, vous lui donnez un ensemble d'instructions \xe0 ex\xe9cuter, mais avec LLMS, c'est une histoire compl\xe8tement diff\xe9rente, vous apprenez \xe0 l'ordinateur non pas comment faire les choses mais comment apprendre \xe0 faire les choses et c'est une approche beaucoup plus flexible.  et est vraiment bon pour de nombreuses applications diff\xe9rentes o\xf9 le codage traditionnel ne pouvait pas les accomplir, donc un exemple d' application est la reconnaissance d'image avec reconnaissance d'image. La programmation traditionnelle vous obligerait \xe0 coder en dur chaque r\xe8gle pour savoir comment, disons, identifier diff\xe9rentes lettres, donc a b c d mais si  vous \xe9crivez ces lettres \xe0 la main, les lettres manuscrites de chacun sont diff\xe9rentes, alors comment utiliser la programmation traditionnelle pour identifier toutes les variations possibles et c'est l\xe0 qu'intervient cette approche de l'IA au lieu de donner \xe0 un ordinateur des instructions explicites sur la fa\xe7on d'identifier une lettre manuscrite que vous lui donnez \xe0 la place  un tas d'exemples de ce \xe0 quoi ressemblent les lettres manuscrites, puis il peut d\xe9duire \xe0 quoi ressemble une nouvelle lettre manuscrite sur la base de tous les exemples qu'elle contient, ce qui distingue \xe9galement l'apprentissage automatique et les grands mod\xe8les de langage et cette nouvelle approche de la programmation est qu'ils  sont beaucoup plus flexibles, beaucoup plus adaptables, ce qui signifie qu'ils peuvent apprendre de leurs erreurs et de leurs inexactitudes et sont donc beaucoup plus \xe9volutifs que la programmation traditionnelle. Les llms sont incroyablement puissants pour un large \xe9ventail de t\xe2ches, y compris la g\xe9n\xe9ration de texte de synth\xe8se, l' \xe9criture cr\xe9ative, la programmation de questions et r\xe9ponses et si vous  J'ai regard\xe9 n'importe laquelle de mes vid\xe9os, vous savez \xe0 quel point ces grands mod\xe8les de langage peuvent \xeatre puissants et ils ne font que mieux savoir qu'\xe0 l' heure actuelle, les grands mod\xe8les de langage et en g\xe9n\xe9ral sont les pires qu'ils auront jamais \xe9t\xe9 et \xe0 mesure que nous en g\xe9n\xe9rons davantage.  donn\xe9es sur Internet et comme nous utilisons des donn\xe9es synth\xe9tiques, ce qui signifie des donn\xe9es cr\xe9\xe9es par d'autres grands mod\xe8les de langage, ces mod\xe8les vont s'am\xe9liorer rapidement et c'est super excitant de penser \xe0 ce que l'avenir nous r\xe9serve maintenant, parlons un peu de l'histoire et de l'\xe9volution de  grands mod\xe8les de langage, nous allons couvrir quelques-uns des grands mod\xe8les de langage aujourd'hui dans cette section. L'histoire des LLM remonte au mod\xe8le Eliza qui date de 1966 et qui \xe9tait en r\xe9alit\xe9 le premier mod\xe8le de langage qu'il avait avant. des r\xe9ponses programm\xe9es bas\xe9es sur des mots-cl\xe9s, il avait une compr\xe9hension tr\xe8s limit\xe9e de la langue anglaise et, comme beaucoup des premiers mod\xe8les de langage, vous avez commenc\xe9 \xe0 voir des trous dans sa logique apr\xe8s quelques allers-retours dans une conversation, puis apr\xe8s cela, les mod\xe8les de langage n'ont vraiment pas \xe9volu\xe9 pour tr\xe8s longtemps, m\xeame si techniquement le premier r\xe9seau de neurones r\xe9current a \xe9t\xe9 cr\xe9\xe9 en 1924 ou RNN, ils n'ont pas vraiment pu apprendre avant 1972 et ces nouveaux mod\xe8les de langage d'apprentissage sont une s\xe9rie de r\xe9seaux de neurones avec des couches et des poids et tout un tas de choses qui  Je ne vais pas entrer dans les d\xe9tails de cette vid\xe9o et les RNN \xe9taient vraiment la premi\xe8re technologie capable de pr\xe9dire le mot suivant dans une phrase plut\xf4t que d'avoir tout pr\xe9programm\xe9 pour cela et c'\xe9tait vraiment la base de la fa\xe7on dont les grands mod\xe8les de langage actuels  travail et m\xeame apr\xe8s cela et l'av\xe8nement de l'apprentissage profond au d\xe9but des ann\xe9es 2000, le domaine de l' IA a \xe9volu\xe9 tr\xe8s lentement avec des mod\xe8les de langage bien en retard sur ce que nous voyons aujourd'hui. Tout a chang\xe9 en 2017, lorsque l' \xe9quipe Google Deep Mind a publi\xe9 un document de recherche sur une nouvelle technologie.  appel\xe9 Transformers et cet article a attir\xe9 votre attention, c'est tout ce dont vous avez besoin et une petite note. Je ne pense m\xeame pas que Google savait vraiment ce qu'ils avaient publi\xe9 \xe0 ce moment-l\xe0, mais ce m\xeame article est ce qui a conduit l' IA ouverte \xe0 d\xe9velopper le chat GPT, donc \xe9videmment un autre ordinateur  les scientifiques ont vu le potentiel de l' architecture Transformers avec cette nouvelle architecture Transformers, elle \xe9tait beaucoup plus avanc\xe9e, elle n\xe9cessitait un temps de formation r\xe9duit et elle avait de nombreuses autres fonctionnalit\xe9s comme l'auto- attention que j'aborderai plus tard dans cette vid\xe9o. Les Transformers permettaient de grands mod\xe8les de langage pr\xe9-entra\xeen\xe9s.  comme gpt1 qui a \xe9t\xe9 d\xe9velopp\xe9 par open AI en 2018, il avait 117 millions de param\xe8tres et c'\xe9tait compl\xe8tement r\xe9volutionnaire mais bient\xf4t surclass\xe9 par d'autres llms puis apr\xe8s cela Bert est sorti en 2018 qui avait 340 millions de param\xe8tres et avait une bidirectionnalit\xe9, ce qui signifie qu'il avait  la capacit\xe9 de traiter le texte dans les deux sens, ce qui l'a aid\xe9 \xe0 mieux comprendre le contexte et, \xe0 titre de comparaison, un mod\xe8le unidirectionnel ne comprend que les mots qui pr\xe9c\xe8dent le texte cible et apr\xe8s ce film, n'a pas d\xe9velopp\xe9 beaucoup de nouvelles technologies, mais  leur \xe9chelle a consid\xe9rablement augment\xe9. Gpt2 a \xe9t\xe9 publi\xe9 d\xe9but 2019 et comportait 2,5 milliards de param\xe8tres, puis GPT 3 en juin 2020 avec 175 milliards de param\xe8tres et c'est \xe0 ce stade que le public a commenc\xe9 \xe0 remarquer les grands mod\xe8les de langage. GPT avait une bien meilleure compr\xe9hension du langage naturel.  langage que n'importe lequel de ses pr\xe9d\xe9cesseurs et c'est le type de mod\xe8le qui alimente le chat GPT qui est probablement le mod\xe8le que vous connaissez le plus et le chat GPT est devenu si populaire parce qu'il \xe9tait beaucoup plus pr\xe9cis que tout ce que quiconque avait jamais vu auparavant et  c'\xe9tait vraiment \xe0 cause de sa taille et parce qu'il \xe9tait d\xe9sormais int\xe9gr\xe9 \xe0 ce format de chatbot, tout le monde pouvait s'y lancer et vraiment comprendre comment interagir avec ce mod\xe8le. Chad GPT 3.5 est sorti en d\xe9cembre 2022 et a lanc\xe9 cette vague actuelle d'IA que nous voyons  aujourd'hui puis en mars 2023, GPT 4 a \xe9t\xe9 publi\xe9 et c'\xe9tait incroyable et c'est toujours incroyable \xe0 ce jour, il avait un \xe9norme 1,76 billion de param\xe8tres signal\xe9s et utilise probablement un m\xe9lange d' approches d'experts, ce qui signifie qu'il dispose de plusieurs mod\xe8les qui sont tous affin\xe9s pour des besoins sp\xe9cifiques.  cas d'utilisation, puis lorsque quelqu'un lui pose une question, il choisit lequel de ces mod\xe8les utiliser, puis il ajoute la multimodalit\xe9 et un tas d'autres fonctionnalit\xe9s et cela nous am\xe8ne l\xe0 o\xf9 nous en sommes aujourd'hui. Parlons maintenant de la fa\xe7on dont les llms fonctionnent r\xe9ellement. un peu plus en d\xe9tail, le processus de fonctionnement des grands mod\xe8les de langage peut \xeatre divis\xe9 en trois \xe9tapes, la premi\xe8re de ces \xe9tapes est appel\xe9e tokenisation et il existe des r\xe9seaux de neurones qui sont form\xe9s pour diviser un texte long en jetons individuels et un jeton repr\xe9sente essentiellement environ 34 secondes de  un mot donc si c'est un mot plus court comme high ou \xe7a ou l\xe0, ce n'est probablement qu'un jeton, mais si vous avez un mot plus long comme r\xe9sum\xe9, il sera divis\xe9 en plusieurs morceaux et la fa\xe7on dont la tokenisation se produit est en fait diff\xe9rente pour chaque mod\xe8le, certains d'entre eux  les pr\xe9fixes et suffixes s\xe9par\xe9s, regardons un exemple quel est le b\xe2timent le plus haut, donc quel est le b\xe2timent le plus haut sont tous des jetons s\xe9par\xe9s et donc cela s\xe9pare le suffixe du b\xe2timent le plus haut mais pas du b\xe2timent car il prend en compte le contexte et cette \xe9tape est effectu\xe9e  Ainsi, les mod\xe8les peuvent comprendre chaque mot individuellement, tout comme les humains, nous comprenons chaque mot individuellement et en tant que groupements de mots, puis la deuxi\xe8me \xe9tape du LLMS est quelque chose appel\xe9 int\xe9grations. Les grands mod\xe8les de langage transforment ces jetons en vecteurs d'int\xe9gration, transformant ces jetons en essentiellement un tas de valeurs num\xe9riques. repr\xe9sentations de ces num\xe9ros de jetons, ce qui permet \xe0 l'ordinateur de lire et de comprendre beaucoup plus facilement chaque mot et comment les diff\xe9rents mots sont li\xe9s les uns aux autres. Ces nombres correspondent tous \xe0 la position dans une base de donn\xe9es vectorielle d'int\xe9gration, puis \xe0 l' \xe9tape finale du processus.  est Transformers auquel nous reviendrons dans un instant, mais parlons d'abord des bases de donn\xe9es vectorielles et je vais utiliser les termes mot et jeton de mani\xe8re interchangeable, alors gardez cela \xe0 l'esprit car ce sont presque la m\xeame chose, pas tout \xe0 fait mais presque.  et donc ces int\xe9grations de mots dont j'ai parl\xe9 sont plac\xe9es dans quelque chose appel\xe9 une base de donn\xe9es vectorielle, ces bases de donn\xe9es sont des m\xe9canismes de stockage et de r\xe9cup\xe9ration hautement optimis\xe9s pour les vecteurs et encore une fois, ce ne sont que des nombres, de longues s\xe9ries de nombres, car ils sont convertis en ceux-ci. vecteurs, ils peuvent facilement voir quels mots sont li\xe9s \xe0 d'autres mots en fonction de leur similitude, de leur proximit\xe9 en fonction de leurs incorporations et c'est ainsi que le grand mod\xe8le de langage est capable de pr\xe9dire le mot suivant en fonction des mots pr\xe9c\xe9dents. Les bases de donn\xe9es vectorielles capturent le  relation entre les donn\xe9es en tant que vecteurs dans un espace multidimensionnel Je sais que cela semble compliqu\xe9, mais ce sont en r\xe9alit\xe9 beaucoup de nombres. Les vecteurs sont des objets avec une ampleur et une direction qui influencent tous deux la similitude d'un vecteur avec un autre et c'est ainsi que les llms repr\xe9sentent les mots bas\xe9s sur ceux-ci.  nombres, chaque mot est transform\xe9 en un vecteur capturant la signification s\xe9mantique et sa relation avec d'autres mots. Voici donc un exemple des mots livre et ver qui, ind\xe9pendamment, peuvent ne pas sembler li\xe9s les uns aux autres, mais ils sont des concepts li\xe9s car ils apparaissent fr\xe9quemment ensemble.  rat de biblioth\xe8que quelqu'un qui aime beaucoup lire et \xe0 cause de cela, ils auront des int\xe9grations qui se ressemblent et ainsi les mod\xe8les d\xe9veloppent une compr\xe9hension du langage naturel en utilisant ces int\xe9grations et en recherchant la similitude de diff\xe9rents mots, des groupes de mots et tout cela  les relations nuanc\xe9es et le format vectoriel aident les mod\xe8les \xe0 mieux comprendre le langage naturel que les autres formats et vous pouvez en quelque sorte consid\xe9rer tout cela comme une carte si vous avez une carte avec deux points de rep\xe8re proches l'un de l'autre, ils seront probablement tr\xe8s similaires  coordonn\xe9es donc c'est un peu comme \xe7a, ok, parlons maintenant de Transformers mat. Les repr\xe9sentations matricielles peuvent \xeatre cr\xe9\xe9es \xe0 partir de ces vecteurs dont nous venons de parler, cela se fait en extrayant certaines informations des nombres et en pla\xe7ant toutes les informations dans une matrice via  un algorithme appel\xe9 attention multi- t\xeates la sortie de l'algorithme d'attention multi-t\xeates est un ensemble de nombres qui indique au mod\xe8le dans quelle mesure les mots et leur ordre contribuent \xe0 la phrase dans son ensemble, nous transformons la matrice d'entr\xe9e en une matrice de sortie qui sera ensuite  correspondre \xe0 un mot ayant les m\xeames valeurs que cette matrice de sortie, donc fondamentalement, nous prenons cette matrice d'entr\xe9e, la convertissons en matrice de sortie, puis la convertissons en langage naturel et le mot est la sortie finale de tout ce processus, cette transformation est effectu\xe9e par  l'algorithme qui a \xe9t\xe9 cr\xe9\xe9 pendant le processus de formation afin que la compr\xe9hension du mod\xe8le sur la fa\xe7on d'effectuer cette transformation soit bas\xe9e sur toutes ses connaissances selon lesquelles il a \xe9t\xe9 form\xe9 avec tout ce texte. Donn\xe9es provenant d'Internet \xe0 partir de livres, d'articles, etc. et il a appris quelles s\xe9quences de  des mots vont ensemble et leurs mots suivants correspondants en fonction des poids d\xe9termin\xe9s lors de la formation. Les transformateurs utilisent un m\xe9canisme d'attention pour comprendre le contexte des mots dans une phrase. Cela implique des calculs avec le produit scalaire qui est essentiellement un nombre repr\xe9sentant la contribution du mot au mot. phrase, il trouvera la diff\xe9rence entre les produits scalaires des mots et lui donnera des valeurs d'attention plus grandes en cons\xe9quence et il prendra davantage en compte ce mot s'il a une plus grande attention. Parlons maintenant de la fa\xe7on dont les grands mod\xe8les de langage sont r\xe9ellement form\xe9s lors de la premi\xe8re \xe9tape de  entra\xeener un grand mod\xe8le de langage consiste \xe0 collecter les donn\xe9es dont vous avez besoin de beaucoup de donn\xe9es quand je dis des milliards de param\xe8tres qui ne sont qu'une mesure de la quantit\xe9 de donn\xe9es r\xe9ellement utilis\xe9es pour entra\xeener ces mod\xe8les et vous devez trouver un tr\xe8s bon ensemble de donn\xe9es si vous avez  de tr\xe8s mauvaises donn\xe9es entrent dans un mod\xe8le, alors vous allez avoir un tr\xe8s mauvais mod\xe8le dans les ordures, donc si un ensemble de donn\xe9es est incomplet ou biais\xe9, le grand mod\xe8le de langage le sera \xe9galement et les ensembles de donn\xe9es sont \xe9normes, nous parlons de massifs massifs  les quantit\xe9s de donn\xe9es qu'ils r\xe9cup\xe8rent \xe0 partir de pages Web, de livres, de conversations, de publications Reddit, de xposts, de transcriptions YouTube, pratiquement partout o\xf9 nous pouvons obtenir des donn\xe9es textuelles, ces donn\xe9es deviennent si pr\xe9cieuses, permettez-moi de mettre en contexte l' ampleur des ensembles de donn\xe9es dont nous disposons. parlons vraiment, alors voici un petit morceau de texte qui repr\xe9sente 276 jetons, c'est tout maintenant si nous effectuons un zoom arri\xe8re, un pixel repr\xe9sente autant de jetons et voici maintenant une repr\xe9sentation de 285 millions de jetons, soit 0,02% des 1,3 billions de jetons que certains grands  les mod\xe8les de langage doivent s'entra\xeener et il y a toute une science derri\xe8re le pr\xe9traitement des donn\xe9es qui pr\xe9pare les donn\xe9es \xe0 utiliser pour entra\xeener un mod\xe8le, de l' examen de la qualit\xe9 des donn\xe9es \xe0 l'\xe9tiquetage de la coh\xe9rence du nettoyage des donn\xe9es, de la transformation des donn\xe9es et de la r\xe9duction des donn\xe9es, mais je ne vais pas le faire.  allez trop loin et ce pr\xe9-traitement peut prendre beaucoup de temps et cela d\xe9pend du type de machine utilis\xe9e, de la puissance de traitement dont vous disposez, de la taille des donn\xe9es, du nombre d' \xe9tapes de pr\xe9-traitement et de tout un tas d'autres  facteurs qui rendent tr\xe8s difficile de savoir exactement combien de temps le pr\xe9traitement va prendre, mais une chose dont nous savons qu'elle prend beaucoup de temps est que les soci\xe9t\xe9s de formation comme Nvidia construisent du mat\xe9riel sp\xe9cifiquement adapt\xe9 aux math\xe9matiques derri\xe8re les grands mod\xe8les de langage et ce mat\xe9riel.  s'am\xe9liore constamment, les logiciels utilis\xe9s pour traiter ces mod\xe8les s'am\xe9liorent \xe9galement et donc le temps total de traitement des mod\xe8les diminue mais la taille des mod\xe8les augmente et pour former ces mod\xe8les, il est extr\xeamement co\xfbteux car vous avez besoin de beaucoup de puissance de traitement.  l'\xe9lectricit\xe9 et ces puces ne sont pas bon march\xe9 et c'est pourquoi le cours de l'action Nvidia a grimp\xe9 en fl\xe8che, la croissance de leurs revenus a \xe9t\xe9 extraordinaire et donc, avec le processus de formation, nous prenons ces donn\xe9es textuelles pr\xe9trait\xe9es dont nous avons parl\xe9 plus t\xf4t et elles sont introduites dans le mod\xe8le, puis  en utilisant Transformers ou toute autre technologie sur laquelle un mod\xe8le est r\xe9ellement bas\xe9, mais tr\xe8s probablement Transformers, il essaiera de pr\xe9dire le mot suivant en fonction du contexte de ces donn\xe9es et il ajustera les poids du mod\xe8le pour obtenir le meilleur r\xe9sultat possible et ce processus se r\xe9p\xe8te  des millions et des millions de fois, encore et encore jusqu'\xe0 ce que nous atteignions une qualit\xe9 optimale, puis la derni\xe8re \xe9tape est l'\xe9valuation, une petite quantit\xe9 de donn\xe9es est mise de c\xf4t\xe9 pour l'\xe9valuation et le mod\xe8le est test\xe9 sur cet ensemble de donn\xe9es pour en v\xe9rifier les performances, puis le mod\xe8le est ajust\xe9 si n\xe9cessaire la m\xe9trique utilis\xe9e pour d\xe9terminer l'efficacit\xe9 du mod\xe8le s'appelle perplexit\xe9 il comparera deux mots en fonction de leur similarit\xe9 et il donnera un bon score si les mots sont li\xe9s et un mauvais score sinon et puis on utilise aussi rlf apprentissage par renforcement via la r\xe9troaction humaine et c'est \xe0 ce moment-l\xe0 que les utilisateurs ou les testeurs testent r\xe9ellement le mod\xe8le et fournissent des scores positifs ou n\xe9gatifs en fonction du r\xe9sultat, puis une fois de plus, le mod\xe8le est ajust\xe9 si n\xe9cessaire, d'accord, parlons maintenant du r\xe9glage fin, ce qui, je pense, est important. vous allez \xeatre int\xe9ress\xe9 parce que c'est quelque chose que la personne moyenne peut aborder assez facilement, nous avons donc ces grands mod\xe8les de langage populaires qui sont form\xe9s sur des ensembles massifs de donn\xe9es pour d\xe9velopper des capacit\xe9s linguistiques g\xe9n\xe9rales et ces mod\xe8les pr\xe9-entra\xeen\xe9s comme Bert comme GPT  donner aux d\xe9veloppeurs une longueur d'avance par rapport aux mod\xe8les de formation \xe0 partir de z\xe9ro, mais vient ensuite un r\xe9glage fin qui nous permet de prendre ces mod\xe8les bruts, ces mod\xe8les Foundation et de les affiner pour nos cas d'utilisation sp\xe9cifiques, alors r\xe9fl\xe9chissons \xe0 un exemple, disons que vous voulez  mod\xe8le de thon fin pour pouvoir prendre des commandes de pizza pour pouvoir avoir des conversations r\xe9pondre \xe0 des questions sur la pizza et enfin pouvoir permettre aux clients d'acheter de la pizza vous pouvez prendre un ensemble de conversations pr\xe9existantes qui illustrent les allers-retours entre une pizzeria  et une charge de clients qui peaufine un mod\xe8le et puis tout d'un coup, ce mod\xe8le va \xeatre bien meilleur pour avoir des conversations sur la commande de pizza, le mod\xe8le met \xe0 jour les poids pour mieux comprendre certaines questions de terminologie de la pizza, les r\xe9ponses tonifient tout et bien  -le r\xe9glage est beaucoup plus rapide qu'une formation compl\xe8te et il produit une pr\xe9cision beaucoup plus \xe9lev\xe9e et un r\xe9glage fin permet d' affiner les mod\xe8les pr\xe9-entra\xeen\xe9s pour des cas d'utilisation r\xe9els et enfin, vous pouvez prendre un seul mod\xe8le de base et l'affiner n'importe quel nombre  de fois pour un certain nombre de cas d'utilisation et il existe de nombreux excellents services qui vous permettent de le faire et encore une fois, tout d\xe9pend de la qualit\xe9 de vos donn\xe9es, donc si vous disposez d' un tr\xe8s bon ensemble de donn\xe9es, vous allez le faire.  - r\xe9gler un mod\xe8le sur le mod\xe8le va \xeatre vraiment tr\xe8s bien et \xe0 l'inverse, si vous avez un ensemble de donn\xe9es de mauvaise qualit\xe9, cela ne fonctionnera pas aussi bien, laissez-moi faire une pause une seconde et parler d' AI Camp, comme mentionn\xe9 plus t\xf4t dans cette vid\xe9o  tout son contenu, les animations ont \xe9t\xe9 cr\xe9\xe9es en collaboration avec des \xe9tudiants d'AI Camp AI Camp est une exp\xe9rience d'apprentissage pour les \xe9tudiants \xe2g\xe9s de 13 ans et plus. Vous travaillez en petits groupes personnalis\xe9s avec des mentors exp\xe9riment\xe9s. Vous travaillez ensemble pour cr\xe9er un produit d'IA \xe0 l'aide d'un ordinateur PNL.  vision et science des donn\xe9es AI Camp propose \xe0 la fois un programme de 3 semaines et un programme unique pendant l'\xe9t\xe9 qui ne n\xe9cessite aucune exp\xe9rience en programmation. Ils ont \xe9galement un nouveau programme de 10 semaines au cours de l'ann\xe9e scolaire qui est moins intensif que le programme unique et 3-nous.  programmes pour les \xe9tudiants tr\xe8s occup\xe9s. La mission d'AI Camp est de fournir aux \xe9tudiants des connaissances approfondies et une intelligence artificielle qui les positionneront pour \xeatre pr\xeats pour un s\xe9jour dans le monde r\xe9el. Je mettrai un lien vers un article de USA Today dans la description de tout sur le camp d'IA.  mais si vous \xeates \xe9tudiant ou si vous \xeates parent d'un \xe9tudiant de cet \xe2ge, je vous recommande fortement de consulter AI Camp, d'aller sur ai-camp.org pour en savoir plus, parlons maintenant des limites et des d\xe9fis des grands mod\xe8les de langage.  aussi capables que soient les LLM, ils ont encore beaucoup de limites, les mod\xe8les r\xe9cents continuent de s'am\xe9liorer mais ils sont toujours d\xe9fectueux, ils sont incroyablement pr\xe9cieux et comp\xe9tents \xe0 certains \xe9gards, mais ils sont \xe9galement profond\xe9ment d\xe9fectueux dans d'autres comme les math\xe9matiques, la logique et le raisonnement, ils ont encore du mal  la plupart du temps, contre les humains qui comprennent des concepts comme celui-l\xe0 assez facilement, les pr\xe9jug\xe9s et la s\xe9curit\xe9 continuent d'\xeatre un gros probl\xe8me. les grands mod\xe8les de langage sont form\xe9s sur des donn\xe9es cr\xe9\xe9es par des humains qui sont naturellement imparfaites. Les humains ont des opinions sur tout et ces opinions se r\xe9percutent sur celles-ci.  mod\xe8les, ces ensembles de donn\xe9es peuvent inclure des informations pr\xe9judiciables ou biais\xe9es et certaines entreprises vont plus loin dans leurs mod\xe8les et fournissent un certain niveau de censure \xe0 ces mod\xe8les et c'est toute une discussion en soi pour savoir si la censure en vaut la peine ou non. Je sais que beaucoup d'entre vous connaissent d\xe9j\xe0 mon  les opinions \xe0 ce sujet tir\xe9es de mes vid\xe9os pr\xe9c\xe9dentes et une autre grande limitation des LLM sont historiquement qu'ils n'ont des connaissances que jusqu'au point o\xf9 leur formation a eu lieu, mais cela commence \xe0 \xeatre r\xe9solu avec le chat GPT pouvant naviguer sur le Web par exemple Gro \xe0 partir de x  .  aai \xe9tant capable d'acc\xe9der aux tweets en direct, mais il y a encore beaucoup de probl\xe8mes \xe0 r\xe9soudre. Un autre autre grand d\xe9fi pour les grands mod\xe8les de langage est les hallucinations, ce qui signifie qu'ils inventent parfois des choses ou se trompent manifestement et ils le seront.  confiants d'avoir tort aussi, ils diront les choses avec la plus grande confiance, mais ils auront compl\xe8tement tort, regardez cet exemple combien de lettres il y a dans la cha\xeene, puis nous lui donnons une cha\xeene de caract\xe8res al\xe9atoire et la r\xe9ponse est que la cha\xeene a m\xeame 16 lettres  bien qu'il ne contienne que 15 lettres, un autre probl\xe8me est que les grands mod\xe8les de langage sont extr\xeamement gourmands en mat\xe9riel, ils co\xfbtent tr\xe8s cher \xe0 former et \xe0 affiner car cela prend beaucoup de puissance de traitement pour le faire et il y a beaucoup d'\xe9thique \xe0 prendre en compte aussi  des entreprises d'IA d\xe9clarent qu'elles n'entra\xeenent pas leurs mod\xe8les sur du mat\xe9riel prot\xe9g\xe9 par le droit d'auteur, mais cela s'est av\xe9r\xe9 faux. Actuellement, de nombreuses poursuites sont en cours devant les tribunaux \xe0 ce sujet. Parlons ensuite des applications r\xe9elles des grands mod\xe8les de langage. Pourquoi sont-ils  ils sont si pr\xe9cieux, pourquoi on en parle tant et pourquoi transforment-ils le monde sous nos yeux de grands mod\xe8les de langage peuvent \xeatre utilis\xe9s pour une grande vari\xe9t\xe9 de t\xe2ches, pas seulement des chatbots, ils peuvent \xeatre utilis\xe9s pour la traduction linguistique, ils peuvent \xeatre utilis\xe9s pour  codage, ils peuvent \xeatre utilis\xe9s comme assistants de programmation, ils peuvent \xeatre utilis\xe9s pour r\xe9sumer les questions, r\xe9pondre \xe0 la r\xe9daction d'essais, traduire et m\xeame cr\xe9er des images et des vid\xe9os. Fondamentalement, tout type de probl\xe8me de pens\xe9e qu'un humain peut r\xe9soudre avec un ordinateur. De grands mod\xe8les de langage peuvent probablement aussi le faire, sinon aujourd'hui. bient\xf4t dans le futur, parlons maintenant des avanc\xe9es et de la recherche actuelles. Actuellement, on parle beaucoup de distillation des connaissances, ce qui signifie essentiellement transf\xe9rer des connaissances cl\xe9s de tr\xe8s grands mod\xe8les de pointe vers des mod\xe8les plus petits et plus efficaces. Pensez- y comme un professeur condensant des d\xe9cennies d'exp\xe9rience dans un  manuel jusqu'\xe0 quelque chose que les \xe9tudiants peuvent comprendre, ce qui permet \xe0 des mod\xe8les de langage plus petits de b\xe9n\xe9ficier des connaissances acquises gr\xe2ce \xe0 ces grands mod\xe8les de langage, tout en fonctionnant toujours de mani\xe8re tr\xe8s efficace sur le mat\xe9riel grand public de tous les jours. Cela rend les grands mod\xe8les de langage plus accessibles et plus pratiques \xe0 ex\xe9cuter, m\xeame sur  sur les t\xe9l\xe9phones portables ou autres appareils finaux, il y a \xe9galement eu beaucoup de recherches et l'accent a \xe9t\xe9 mis sur rag, qui est une g\xe9n\xe9ration augment\xe9e de r\xe9cup\xe9ration, ce qui signifie essentiellement que vous donnez \xe0 de grands mod\xe8les de langage la possibilit\xe9 de rechercher des informations en dehors des donn\xe9es sur lesquelles ils ont \xe9t\xe9 form\xe9s.  en utilisant les bases de donn\xe9es Vector de la m\xeame mani\xe8re que les grands mod\xe8les de langage sont form\xe9s, mais vous \xeates capable de stocker d'\xe9normes quantit\xe9s de donn\xe9es suppl\xe9mentaires qui peuvent \xeatre interrog\xe9es par ce grand mod\xe8le de langage, parlons maintenant des consid\xe9rations \xe9thiques et il y a beaucoup de choses \xe0 penser ici et moi  J'aborde simplement certains des sujets majeurs dont nous avons d\xe9j\xe0 parl\xe9, \xe0 savoir que les mod\xe8les sont form\xe9s sur du mat\xe9riel potentiellement prot\xe9g\xe9 par le droit d'auteur et si tel est le cas, l'utilisation \xe9quitable ne sera probablement pas la prochaine \xe9tape. Ces mod\xe8les peuvent et seront utilis\xe9s pour des actes nuisibles, il n'y a pas moyen de l' \xe9viter.  les grands mod\xe8les de langage peuvent \xeatre utilis\xe9s pour arnaquer d'autres personnes afin de cr\xe9er des campagnes massives de d\xe9sinformation et de d\xe9sinformation, y compris de fausses images, de faux textes, de fausses opinions et il est presque certain que l'ensemble des cols blancs sera perturb\xe9 par de grands mod\xe8les de langage, comme je l'ai mentionn\xe9, tout ce que n'importe qui peut faire devant  d'un ordinateur est probablement quelque chose que l'IA peut aussi faire, donc avocats, \xe9crivains, programmeurs, il y a tellement de professions diff\xe9rentes qui vont \xeatre compl\xe8tement perturb\xe9es par l' intelligence artificielle et puis finalement AGI, que se passe-t-il lorsque l'IA devient si intelligente et peut-\xeatre m\xeame commence \xe0 penser par elle-m\xeame  c'est l\xe0 que nous devons avoir quelque chose appel\xe9 alignement, ce qui signifie que l'IA est align\xe9e sur les m\xeames incitations et r\xe9sultats que les humains. Parlons donc pour la derni\xe8re fois de ce qui se passe sur The Cutting Edge et dans un avenir imm\xe9diat, il existe un certain nombre de fa\xe7ons dont les grands mod\xe8les de langage peuvent  \xeatre am\xe9lior\xe9s, ils peuvent d'abord v\xe9rifier eux-m\xeames les faits avec les informations recueillies sur le Web, mais vous pouvez \xe9videmment voir les d\xe9fauts inh\xe9rents \xe0 cela, puis nous avons \xe9galement abord\xe9 le m\xe9lange d'experts qui est une nouvelle technologie incroyable qui permet \xe0 plusieurs mod\xe8les d'\xeatre en quelque sorte fusionn\xe9s, tout va bien. Adaptez-vous pour \xeatre des experts dans certains domaines, puis lorsque l'invite r\xe9elle arrive, il choisit lequel de ces experts utiliser, ce sont donc d'\xe9normes mod\xe8les qui fonctionnent vraiment tr\xe8s efficacement et puis il y a beaucoup de travail sur la multimodalit\xe9, donc en prenant la contribution de la voix \xe0 partir des images  \xe0 partir de la vid\xe9o, de toutes les sources d'entr\xe9e possibles et d'avoir une seule sortie \xe0 partir de celle-ci, beaucoup de travail est \xe9galement fait pour am\xe9liorer la capacit\xe9 de raisonnement, faire r\xe9fl\xe9chir les mod\xe8les lentement est une nouvelle tendance que j'ai vue dans des articles comme orca \xe9galement, qui force simplement un grand mod\xe8le de langage pour r\xe9fl\xe9chir aux probl\xe8mes \xe9tape par \xe9tape plut\xf4t que d'essayer de sauter imm\xe9diatement \xe0 la conclusion finale, puis \xe9galement \xe0 des tailles de contexte plus grandes si vous souhaitez qu'un grand mod\xe8le de langage traite une \xe9norme quantit\xe9 de donn\xe9es, il doit avoir une tr\xe8s grande fen\xeatre de contexte et un La fen\xeatre contextuelle indique la quantit\xe9 d'informations que vous pouvez donner \xe0 une invite pour obtenir le r\xe9sultat. Une fa\xe7on d'y parvenir est de donner de la m\xe9moire \xe0 de grands mod\xe8les de langage avec des projets comme mgpt sur lesquels j'ai fait une vid\xe9o et je vais le laisser dans la description ci-dessous.  et cela signifie simplement donner aux mod\xe8les une m\xe9moire externe \xe0 partir de cet ensemble de donn\xe9es de base sur lequel ils ont \xe9t\xe9 form\xe9s, donc c'est tout pour aujourd'hui si vous avez aim\xe9 cette vid\xe9o, pensez \xe0 donner un like et abonnez-vous, consultez AI Camp, je laisserai toutes les informations dans la description ci-dessous  et bien s\xfbr, regardez l'une de mes autres vid\xe9os sur l'IA si vous voulez en savoir encore plus, je vous verrai dans la prochaine\"",source:"@site/docs/video_transcription.md",sourceDirName:".",slug:"/video_transcription",permalink:"/igora/docs/video_transcription",draft:!1,unlisted:!1,editUrl:"https://github.com/scenaristeur/igora/tree/main/website/docs/video_transcription.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Usage",permalink:"/igora/docs/usage"}},u={},l=[];function d(e){const s={p:"p",...(0,a.R)(),...e.components};return(0,t.jsx)(s.p,{children:"cette vid\xe9o va vous donner tout ce dont vous avez besoin pour ne rien savoir de l'intelligence artificielle et des grands mod\xe8les de langage et avoir une base solide sur le fonctionnement de ces technologies r\xe9volutionnaires au cours de l'ann\xe9e \xe9coul\xe9e. L' intelligence artificielle a compl\xe8tement chang\xe9 le monde avec des produits comme chat PT potentiellement annexant chaque industrie et la fa\xe7on dont les gens interagissent avec la technologie en g\xe9n\xe9ral et dans cette vid\xe9o, je me concentrerai sur la fa\xe7on dont ils fonctionnent, les applications d'it\xe9rations et bien plus encore et cette vid\xe9o a \xe9t\xe9 cr\xe9\xe9e en collaboration avec un programme incroyable appel\xe9 AI camp dans lequel les lyc\xe9ens apprennent tout sur l' intelligence artificielle et j'en parlerai davantage plus tard dans la vid\xe9o, commen\xe7ons par qu'est-ce qu'un LLM, est-il diff\xe9rent de l'IA et en quoi le chat GPT est-il li\xe9 \xe0 tout cela ? Les LLM repr\xe9sentent de grands mod\xe8les de langage qui  est un type de r\xe9seau neuronal form\xe9 sur d'\xe9normes quantit\xe9s de donn\xe9es textuelles. Il est g\xe9n\xe9ralement form\xe9 sur des donn\xe9es qui peuvent \xeatre trouv\xe9es en ligne, du web scraping aux livres en passant par les transcriptions. Tout ce qui est bas\xe9 sur du texte peut \xeatre form\xe9 dans un grand mod\xe8le de langage et prendre du recul.  qu'est-ce qu'un r\xe9seau de neurones, un r\xe9seau de neurones est essentiellement une s\xe9rie d'algorithmes qui tentent de reconna\xeetre des mod\xe8les dans les donn\xe9es et ce qu'ils essaient en r\xe9alit\xe9 de simuler le fonctionnement du cerveau humain et les LLM sont un type sp\xe9cifique de r\xe9seau de neurones qui se concentre sur la compr\xe9hension.  le langage naturel et, comme mentionn\xe9, les llms apprennent en lisant des tonnes de livres, d'articles, de textes sur Internet et il n'y a vraiment aucune limitation. En quoi les llms diff\xe8rent-ils de la programmation traditionnelle? La programmation traditionnelle est bas\xe9e sur des instructions, ce qui signifie que si x, alors pourquoi vous dites explicitement \xe0 l' ordinateur  que faire, vous lui donnez un ensemble d'instructions \xe0 ex\xe9cuter, mais avec LLMS, c'est une histoire compl\xe8tement diff\xe9rente, vous apprenez \xe0 l'ordinateur non pas comment faire les choses mais comment apprendre \xe0 faire les choses et c'est une approche beaucoup plus flexible.  et est vraiment bon pour de nombreuses applications diff\xe9rentes o\xf9 le codage traditionnel ne pouvait pas les accomplir, donc un exemple d' application est la reconnaissance d'image avec reconnaissance d'image. La programmation traditionnelle vous obligerait \xe0 coder en dur chaque r\xe8gle pour savoir comment, disons, identifier diff\xe9rentes lettres, donc a b c d mais si  vous \xe9crivez ces lettres \xe0 la main, les lettres manuscrites de chacun sont diff\xe9rentes, alors comment utiliser la programmation traditionnelle pour identifier toutes les variations possibles et c'est l\xe0 qu'intervient cette approche de l'IA au lieu de donner \xe0 un ordinateur des instructions explicites sur la fa\xe7on d'identifier une lettre manuscrite que vous lui donnez \xe0 la place  un tas d'exemples de ce \xe0 quoi ressemblent les lettres manuscrites, puis il peut d\xe9duire \xe0 quoi ressemble une nouvelle lettre manuscrite sur la base de tous les exemples qu'elle contient, ce qui distingue \xe9galement l'apprentissage automatique et les grands mod\xe8les de langage et cette nouvelle approche de la programmation est qu'ils  sont beaucoup plus flexibles, beaucoup plus adaptables, ce qui signifie qu'ils peuvent apprendre de leurs erreurs et de leurs inexactitudes et sont donc beaucoup plus \xe9volutifs que la programmation traditionnelle. Les llms sont incroyablement puissants pour un large \xe9ventail de t\xe2ches, y compris la g\xe9n\xe9ration de texte de synth\xe8se, l' \xe9criture cr\xe9ative, la programmation de questions et r\xe9ponses et si vous  J'ai regard\xe9 n'importe laquelle de mes vid\xe9os, vous savez \xe0 quel point ces grands mod\xe8les de langage peuvent \xeatre puissants et ils ne font que mieux savoir qu'\xe0 l' heure actuelle, les grands mod\xe8les de langage et en g\xe9n\xe9ral sont les pires qu'ils auront jamais \xe9t\xe9 et \xe0 mesure que nous en g\xe9n\xe9rons davantage.  donn\xe9es sur Internet et comme nous utilisons des donn\xe9es synth\xe9tiques, ce qui signifie des donn\xe9es cr\xe9\xe9es par d'autres grands mod\xe8les de langage, ces mod\xe8les vont s'am\xe9liorer rapidement et c'est super excitant de penser \xe0 ce que l'avenir nous r\xe9serve maintenant, parlons un peu de l'histoire et de l'\xe9volution de  grands mod\xe8les de langage, nous allons couvrir quelques-uns des grands mod\xe8les de langage aujourd'hui dans cette section. L'histoire des LLM remonte au mod\xe8le Eliza qui date de 1966 et qui \xe9tait en r\xe9alit\xe9 le premier mod\xe8le de langage qu'il avait avant. des r\xe9ponses programm\xe9es bas\xe9es sur des mots-cl\xe9s, il avait une compr\xe9hension tr\xe8s limit\xe9e de la langue anglaise et, comme beaucoup des premiers mod\xe8les de langage, vous avez commenc\xe9 \xe0 voir des trous dans sa logique apr\xe8s quelques allers-retours dans une conversation, puis apr\xe8s cela, les mod\xe8les de langage n'ont vraiment pas \xe9volu\xe9 pour tr\xe8s longtemps, m\xeame si techniquement le premier r\xe9seau de neurones r\xe9current a \xe9t\xe9 cr\xe9\xe9 en 1924 ou RNN, ils n'ont pas vraiment pu apprendre avant 1972 et ces nouveaux mod\xe8les de langage d'apprentissage sont une s\xe9rie de r\xe9seaux de neurones avec des couches et des poids et tout un tas de choses qui  Je ne vais pas entrer dans les d\xe9tails de cette vid\xe9o et les RNN \xe9taient vraiment la premi\xe8re technologie capable de pr\xe9dire le mot suivant dans une phrase plut\xf4t que d'avoir tout pr\xe9programm\xe9 pour cela et c'\xe9tait vraiment la base de la fa\xe7on dont les grands mod\xe8les de langage actuels  travail et m\xeame apr\xe8s cela et l'av\xe8nement de l'apprentissage profond au d\xe9but des ann\xe9es 2000, le domaine de l' IA a \xe9volu\xe9 tr\xe8s lentement avec des mod\xe8les de langage bien en retard sur ce que nous voyons aujourd'hui. Tout a chang\xe9 en 2017, lorsque l' \xe9quipe Google Deep Mind a publi\xe9 un document de recherche sur une nouvelle technologie.  appel\xe9 Transformers et cet article a attir\xe9 votre attention, c'est tout ce dont vous avez besoin et une petite note. Je ne pense m\xeame pas que Google savait vraiment ce qu'ils avaient publi\xe9 \xe0 ce moment-l\xe0, mais ce m\xeame article est ce qui a conduit l' IA ouverte \xe0 d\xe9velopper le chat GPT, donc \xe9videmment un autre ordinateur  les scientifiques ont vu le potentiel de l' architecture Transformers avec cette nouvelle architecture Transformers, elle \xe9tait beaucoup plus avanc\xe9e, elle n\xe9cessitait un temps de formation r\xe9duit et elle avait de nombreuses autres fonctionnalit\xe9s comme l'auto- attention que j'aborderai plus tard dans cette vid\xe9o. Les Transformers permettaient de grands mod\xe8les de langage pr\xe9-entra\xeen\xe9s.  comme gpt1 qui a \xe9t\xe9 d\xe9velopp\xe9 par open AI en 2018, il avait 117 millions de param\xe8tres et c'\xe9tait compl\xe8tement r\xe9volutionnaire mais bient\xf4t surclass\xe9 par d'autres llms puis apr\xe8s cela Bert est sorti en 2018 qui avait 340 millions de param\xe8tres et avait une bidirectionnalit\xe9, ce qui signifie qu'il avait  la capacit\xe9 de traiter le texte dans les deux sens, ce qui l'a aid\xe9 \xe0 mieux comprendre le contexte et, \xe0 titre de comparaison, un mod\xe8le unidirectionnel ne comprend que les mots qui pr\xe9c\xe8dent le texte cible et apr\xe8s ce film, n'a pas d\xe9velopp\xe9 beaucoup de nouvelles technologies, mais  leur \xe9chelle a consid\xe9rablement augment\xe9. Gpt2 a \xe9t\xe9 publi\xe9 d\xe9but 2019 et comportait 2,5 milliards de param\xe8tres, puis GPT 3 en juin 2020 avec 175 milliards de param\xe8tres et c'est \xe0 ce stade que le public a commenc\xe9 \xe0 remarquer les grands mod\xe8les de langage. GPT avait une bien meilleure compr\xe9hension du langage naturel.  langage que n'importe lequel de ses pr\xe9d\xe9cesseurs et c'est le type de mod\xe8le qui alimente le chat GPT qui est probablement le mod\xe8le que vous connaissez le plus et le chat GPT est devenu si populaire parce qu'il \xe9tait beaucoup plus pr\xe9cis que tout ce que quiconque avait jamais vu auparavant et  c'\xe9tait vraiment \xe0 cause de sa taille et parce qu'il \xe9tait d\xe9sormais int\xe9gr\xe9 \xe0 ce format de chatbot, tout le monde pouvait s'y lancer et vraiment comprendre comment interagir avec ce mod\xe8le. Chad GPT 3.5 est sorti en d\xe9cembre 2022 et a lanc\xe9 cette vague actuelle d'IA que nous voyons  aujourd'hui puis en mars 2023, GPT 4 a \xe9t\xe9 publi\xe9 et c'\xe9tait incroyable et c'est toujours incroyable \xe0 ce jour, il avait un \xe9norme 1,76 billion de param\xe8tres signal\xe9s et utilise probablement un m\xe9lange d' approches d'experts, ce qui signifie qu'il dispose de plusieurs mod\xe8les qui sont tous affin\xe9s pour des besoins sp\xe9cifiques.  cas d'utilisation, puis lorsque quelqu'un lui pose une question, il choisit lequel de ces mod\xe8les utiliser, puis il ajoute la multimodalit\xe9 et un tas d'autres fonctionnalit\xe9s et cela nous am\xe8ne l\xe0 o\xf9 nous en sommes aujourd'hui. Parlons maintenant de la fa\xe7on dont les llms fonctionnent r\xe9ellement. un peu plus en d\xe9tail, le processus de fonctionnement des grands mod\xe8les de langage peut \xeatre divis\xe9 en trois \xe9tapes, la premi\xe8re de ces \xe9tapes est appel\xe9e tokenisation et il existe des r\xe9seaux de neurones qui sont form\xe9s pour diviser un texte long en jetons individuels et un jeton repr\xe9sente essentiellement environ 34 secondes de  un mot donc si c'est un mot plus court comme high ou \xe7a ou l\xe0, ce n'est probablement qu'un jeton, mais si vous avez un mot plus long comme r\xe9sum\xe9, il sera divis\xe9 en plusieurs morceaux et la fa\xe7on dont la tokenisation se produit est en fait diff\xe9rente pour chaque mod\xe8le, certains d'entre eux  les pr\xe9fixes et suffixes s\xe9par\xe9s, regardons un exemple quel est le b\xe2timent le plus haut, donc quel est le b\xe2timent le plus haut sont tous des jetons s\xe9par\xe9s et donc cela s\xe9pare le suffixe du b\xe2timent le plus haut mais pas du b\xe2timent car il prend en compte le contexte et cette \xe9tape est effectu\xe9e  Ainsi, les mod\xe8les peuvent comprendre chaque mot individuellement, tout comme les humains, nous comprenons chaque mot individuellement et en tant que groupements de mots, puis la deuxi\xe8me \xe9tape du LLMS est quelque chose appel\xe9 int\xe9grations. Les grands mod\xe8les de langage transforment ces jetons en vecteurs d'int\xe9gration, transformant ces jetons en essentiellement un tas de valeurs num\xe9riques. repr\xe9sentations de ces num\xe9ros de jetons, ce qui permet \xe0 l'ordinateur de lire et de comprendre beaucoup plus facilement chaque mot et comment les diff\xe9rents mots sont li\xe9s les uns aux autres. Ces nombres correspondent tous \xe0 la position dans une base de donn\xe9es vectorielle d'int\xe9gration, puis \xe0 l' \xe9tape finale du processus.  est Transformers auquel nous reviendrons dans un instant, mais parlons d'abord des bases de donn\xe9es vectorielles et je vais utiliser les termes mot et jeton de mani\xe8re interchangeable, alors gardez cela \xe0 l'esprit car ce sont presque la m\xeame chose, pas tout \xe0 fait mais presque.  et donc ces int\xe9grations de mots dont j'ai parl\xe9 sont plac\xe9es dans quelque chose appel\xe9 une base de donn\xe9es vectorielle, ces bases de donn\xe9es sont des m\xe9canismes de stockage et de r\xe9cup\xe9ration hautement optimis\xe9s pour les vecteurs et encore une fois, ce ne sont que des nombres, de longues s\xe9ries de nombres, car ils sont convertis en ceux-ci. vecteurs, ils peuvent facilement voir quels mots sont li\xe9s \xe0 d'autres mots en fonction de leur similitude, de leur proximit\xe9 en fonction de leurs incorporations et c'est ainsi que le grand mod\xe8le de langage est capable de pr\xe9dire le mot suivant en fonction des mots pr\xe9c\xe9dents. Les bases de donn\xe9es vectorielles capturent le  relation entre les donn\xe9es en tant que vecteurs dans un espace multidimensionnel Je sais que cela semble compliqu\xe9, mais ce sont en r\xe9alit\xe9 beaucoup de nombres. Les vecteurs sont des objets avec une ampleur et une direction qui influencent tous deux la similitude d'un vecteur avec un autre et c'est ainsi que les llms repr\xe9sentent les mots bas\xe9s sur ceux-ci.  nombres, chaque mot est transform\xe9 en un vecteur capturant la signification s\xe9mantique et sa relation avec d'autres mots. Voici donc un exemple des mots livre et ver qui, ind\xe9pendamment, peuvent ne pas sembler li\xe9s les uns aux autres, mais ils sont des concepts li\xe9s car ils apparaissent fr\xe9quemment ensemble.  rat de biblioth\xe8que quelqu'un qui aime beaucoup lire et \xe0 cause de cela, ils auront des int\xe9grations qui se ressemblent et ainsi les mod\xe8les d\xe9veloppent une compr\xe9hension du langage naturel en utilisant ces int\xe9grations et en recherchant la similitude de diff\xe9rents mots, des groupes de mots et tout cela  les relations nuanc\xe9es et le format vectoriel aident les mod\xe8les \xe0 mieux comprendre le langage naturel que les autres formats et vous pouvez en quelque sorte consid\xe9rer tout cela comme une carte si vous avez une carte avec deux points de rep\xe8re proches l'un de l'autre, ils seront probablement tr\xe8s similaires  coordonn\xe9es donc c'est un peu comme \xe7a, ok, parlons maintenant de Transformers mat. Les repr\xe9sentations matricielles peuvent \xeatre cr\xe9\xe9es \xe0 partir de ces vecteurs dont nous venons de parler, cela se fait en extrayant certaines informations des nombres et en pla\xe7ant toutes les informations dans une matrice via  un algorithme appel\xe9 attention multi- t\xeates la sortie de l'algorithme d'attention multi-t\xeates est un ensemble de nombres qui indique au mod\xe8le dans quelle mesure les mots et leur ordre contribuent \xe0 la phrase dans son ensemble, nous transformons la matrice d'entr\xe9e en une matrice de sortie qui sera ensuite  correspondre \xe0 un mot ayant les m\xeames valeurs que cette matrice de sortie, donc fondamentalement, nous prenons cette matrice d'entr\xe9e, la convertissons en matrice de sortie, puis la convertissons en langage naturel et le mot est la sortie finale de tout ce processus, cette transformation est effectu\xe9e par  l'algorithme qui a \xe9t\xe9 cr\xe9\xe9 pendant le processus de formation afin que la compr\xe9hension du mod\xe8le sur la fa\xe7on d'effectuer cette transformation soit bas\xe9e sur toutes ses connaissances selon lesquelles il a \xe9t\xe9 form\xe9 avec tout ce texte. Donn\xe9es provenant d'Internet \xe0 partir de livres, d'articles, etc. et il a appris quelles s\xe9quences de  des mots vont ensemble et leurs mots suivants correspondants en fonction des poids d\xe9termin\xe9s lors de la formation. Les transformateurs utilisent un m\xe9canisme d'attention pour comprendre le contexte des mots dans une phrase. Cela implique des calculs avec le produit scalaire qui est essentiellement un nombre repr\xe9sentant la contribution du mot au mot. phrase, il trouvera la diff\xe9rence entre les produits scalaires des mots et lui donnera des valeurs d'attention plus grandes en cons\xe9quence et il prendra davantage en compte ce mot s'il a une plus grande attention. Parlons maintenant de la fa\xe7on dont les grands mod\xe8les de langage sont r\xe9ellement form\xe9s lors de la premi\xe8re \xe9tape de  entra\xeener un grand mod\xe8le de langage consiste \xe0 collecter les donn\xe9es dont vous avez besoin de beaucoup de donn\xe9es quand je dis des milliards de param\xe8tres qui ne sont qu'une mesure de la quantit\xe9 de donn\xe9es r\xe9ellement utilis\xe9es pour entra\xeener ces mod\xe8les et vous devez trouver un tr\xe8s bon ensemble de donn\xe9es si vous avez  de tr\xe8s mauvaises donn\xe9es entrent dans un mod\xe8le, alors vous allez avoir un tr\xe8s mauvais mod\xe8le dans les ordures, donc si un ensemble de donn\xe9es est incomplet ou biais\xe9, le grand mod\xe8le de langage le sera \xe9galement et les ensembles de donn\xe9es sont \xe9normes, nous parlons de massifs massifs  les quantit\xe9s de donn\xe9es qu'ils r\xe9cup\xe8rent \xe0 partir de pages Web, de livres, de conversations, de publications Reddit, de xposts, de transcriptions YouTube, pratiquement partout o\xf9 nous pouvons obtenir des donn\xe9es textuelles, ces donn\xe9es deviennent si pr\xe9cieuses, permettez-moi de mettre en contexte l' ampleur des ensembles de donn\xe9es dont nous disposons. parlons vraiment, alors voici un petit morceau de texte qui repr\xe9sente 276 jetons, c'est tout maintenant si nous effectuons un zoom arri\xe8re, un pixel repr\xe9sente autant de jetons et voici maintenant une repr\xe9sentation de 285 millions de jetons, soit 0,02% des 1,3 billions de jetons que certains grands  les mod\xe8les de langage doivent s'entra\xeener et il y a toute une science derri\xe8re le pr\xe9traitement des donn\xe9es qui pr\xe9pare les donn\xe9es \xe0 utiliser pour entra\xeener un mod\xe8le, de l' examen de la qualit\xe9 des donn\xe9es \xe0 l'\xe9tiquetage de la coh\xe9rence du nettoyage des donn\xe9es, de la transformation des donn\xe9es et de la r\xe9duction des donn\xe9es, mais je ne vais pas le faire.  allez trop loin et ce pr\xe9-traitement peut prendre beaucoup de temps et cela d\xe9pend du type de machine utilis\xe9e, de la puissance de traitement dont vous disposez, de la taille des donn\xe9es, du nombre d' \xe9tapes de pr\xe9-traitement et de tout un tas d'autres  facteurs qui rendent tr\xe8s difficile de savoir exactement combien de temps le pr\xe9traitement va prendre, mais une chose dont nous savons qu'elle prend beaucoup de temps est que les soci\xe9t\xe9s de formation comme Nvidia construisent du mat\xe9riel sp\xe9cifiquement adapt\xe9 aux math\xe9matiques derri\xe8re les grands mod\xe8les de langage et ce mat\xe9riel.  s'am\xe9liore constamment, les logiciels utilis\xe9s pour traiter ces mod\xe8les s'am\xe9liorent \xe9galement et donc le temps total de traitement des mod\xe8les diminue mais la taille des mod\xe8les augmente et pour former ces mod\xe8les, il est extr\xeamement co\xfbteux car vous avez besoin de beaucoup de puissance de traitement.  l'\xe9lectricit\xe9 et ces puces ne sont pas bon march\xe9 et c'est pourquoi le cours de l'action Nvidia a grimp\xe9 en fl\xe8che, la croissance de leurs revenus a \xe9t\xe9 extraordinaire et donc, avec le processus de formation, nous prenons ces donn\xe9es textuelles pr\xe9trait\xe9es dont nous avons parl\xe9 plus t\xf4t et elles sont introduites dans le mod\xe8le, puis  en utilisant Transformers ou toute autre technologie sur laquelle un mod\xe8le est r\xe9ellement bas\xe9, mais tr\xe8s probablement Transformers, il essaiera de pr\xe9dire le mot suivant en fonction du contexte de ces donn\xe9es et il ajustera les poids du mod\xe8le pour obtenir le meilleur r\xe9sultat possible et ce processus se r\xe9p\xe8te  des millions et des millions de fois, encore et encore jusqu'\xe0 ce que nous atteignions une qualit\xe9 optimale, puis la derni\xe8re \xe9tape est l'\xe9valuation, une petite quantit\xe9 de donn\xe9es est mise de c\xf4t\xe9 pour l'\xe9valuation et le mod\xe8le est test\xe9 sur cet ensemble de donn\xe9es pour en v\xe9rifier les performances, puis le mod\xe8le est ajust\xe9 si n\xe9cessaire la m\xe9trique utilis\xe9e pour d\xe9terminer l'efficacit\xe9 du mod\xe8le s'appelle perplexit\xe9 il comparera deux mots en fonction de leur similarit\xe9 et il donnera un bon score si les mots sont li\xe9s et un mauvais score sinon et puis on utilise aussi rlf apprentissage par renforcement via la r\xe9troaction humaine et c'est \xe0 ce moment-l\xe0 que les utilisateurs ou les testeurs testent r\xe9ellement le mod\xe8le et fournissent des scores positifs ou n\xe9gatifs en fonction du r\xe9sultat, puis une fois de plus, le mod\xe8le est ajust\xe9 si n\xe9cessaire, d'accord, parlons maintenant du r\xe9glage fin, ce qui, je pense, est important. vous allez \xeatre int\xe9ress\xe9 parce que c'est quelque chose que la personne moyenne peut aborder assez facilement, nous avons donc ces grands mod\xe8les de langage populaires qui sont form\xe9s sur des ensembles massifs de donn\xe9es pour d\xe9velopper des capacit\xe9s linguistiques g\xe9n\xe9rales et ces mod\xe8les pr\xe9-entra\xeen\xe9s comme Bert comme GPT  donner aux d\xe9veloppeurs une longueur d'avance par rapport aux mod\xe8les de formation \xe0 partir de z\xe9ro, mais vient ensuite un r\xe9glage fin qui nous permet de prendre ces mod\xe8les bruts, ces mod\xe8les Foundation et de les affiner pour nos cas d'utilisation sp\xe9cifiques, alors r\xe9fl\xe9chissons \xe0 un exemple, disons que vous voulez  mod\xe8le de thon fin pour pouvoir prendre des commandes de pizza pour pouvoir avoir des conversations r\xe9pondre \xe0 des questions sur la pizza et enfin pouvoir permettre aux clients d'acheter de la pizza vous pouvez prendre un ensemble de conversations pr\xe9existantes qui illustrent les allers-retours entre une pizzeria  et une charge de clients qui peaufine un mod\xe8le et puis tout d'un coup, ce mod\xe8le va \xeatre bien meilleur pour avoir des conversations sur la commande de pizza, le mod\xe8le met \xe0 jour les poids pour mieux comprendre certaines questions de terminologie de la pizza, les r\xe9ponses tonifient tout et bien  -le r\xe9glage est beaucoup plus rapide qu'une formation compl\xe8te et il produit une pr\xe9cision beaucoup plus \xe9lev\xe9e et un r\xe9glage fin permet d' affiner les mod\xe8les pr\xe9-entra\xeen\xe9s pour des cas d'utilisation r\xe9els et enfin, vous pouvez prendre un seul mod\xe8le de base et l'affiner n'importe quel nombre  de fois pour un certain nombre de cas d'utilisation et il existe de nombreux excellents services qui vous permettent de le faire et encore une fois, tout d\xe9pend de la qualit\xe9 de vos donn\xe9es, donc si vous disposez d' un tr\xe8s bon ensemble de donn\xe9es, vous allez le faire.  - r\xe9gler un mod\xe8le sur le mod\xe8le va \xeatre vraiment tr\xe8s bien et \xe0 l'inverse, si vous avez un ensemble de donn\xe9es de mauvaise qualit\xe9, cela ne fonctionnera pas aussi bien, laissez-moi faire une pause une seconde et parler d' AI Camp, comme mentionn\xe9 plus t\xf4t dans cette vid\xe9o  tout son contenu, les animations ont \xe9t\xe9 cr\xe9\xe9es en collaboration avec des \xe9tudiants d'AI Camp AI Camp est une exp\xe9rience d'apprentissage pour les \xe9tudiants \xe2g\xe9s de 13 ans et plus. Vous travaillez en petits groupes personnalis\xe9s avec des mentors exp\xe9riment\xe9s. Vous travaillez ensemble pour cr\xe9er un produit d'IA \xe0 l'aide d'un ordinateur PNL.  vision et science des donn\xe9es AI Camp propose \xe0 la fois un programme de 3 semaines et un programme unique pendant l'\xe9t\xe9 qui ne n\xe9cessite aucune exp\xe9rience en programmation. Ils ont \xe9galement un nouveau programme de 10 semaines au cours de l'ann\xe9e scolaire qui est moins intensif que le programme unique et 3-nous.  programmes pour les \xe9tudiants tr\xe8s occup\xe9s. La mission d'AI Camp est de fournir aux \xe9tudiants des connaissances approfondies et une intelligence artificielle qui les positionneront pour \xeatre pr\xeats pour un s\xe9jour dans le monde r\xe9el. Je mettrai un lien vers un article de USA Today dans la description de tout sur le camp d'IA.  mais si vous \xeates \xe9tudiant ou si vous \xeates parent d'un \xe9tudiant de cet \xe2ge, je vous recommande fortement de consulter AI Camp, d'aller sur ai-camp.org pour en savoir plus, parlons maintenant des limites et des d\xe9fis des grands mod\xe8les de langage.  aussi capables que soient les LLM, ils ont encore beaucoup de limites, les mod\xe8les r\xe9cents continuent de s'am\xe9liorer mais ils sont toujours d\xe9fectueux, ils sont incroyablement pr\xe9cieux et comp\xe9tents \xe0 certains \xe9gards, mais ils sont \xe9galement profond\xe9ment d\xe9fectueux dans d'autres comme les math\xe9matiques, la logique et le raisonnement, ils ont encore du mal  la plupart du temps, contre les humains qui comprennent des concepts comme celui-l\xe0 assez facilement, les pr\xe9jug\xe9s et la s\xe9curit\xe9 continuent d'\xeatre un gros probl\xe8me. les grands mod\xe8les de langage sont form\xe9s sur des donn\xe9es cr\xe9\xe9es par des humains qui sont naturellement imparfaites. Les humains ont des opinions sur tout et ces opinions se r\xe9percutent sur celles-ci.  mod\xe8les, ces ensembles de donn\xe9es peuvent inclure des informations pr\xe9judiciables ou biais\xe9es et certaines entreprises vont plus loin dans leurs mod\xe8les et fournissent un certain niveau de censure \xe0 ces mod\xe8les et c'est toute une discussion en soi pour savoir si la censure en vaut la peine ou non. Je sais que beaucoup d'entre vous connaissent d\xe9j\xe0 mon  les opinions \xe0 ce sujet tir\xe9es de mes vid\xe9os pr\xe9c\xe9dentes et une autre grande limitation des LLM sont historiquement qu'ils n'ont des connaissances que jusqu'au point o\xf9 leur formation a eu lieu, mais cela commence \xe0 \xeatre r\xe9solu avec le chat GPT pouvant naviguer sur le Web par exemple Gro \xe0 partir de x  .  aai \xe9tant capable d'acc\xe9der aux tweets en direct, mais il y a encore beaucoup de probl\xe8mes \xe0 r\xe9soudre. Un autre autre grand d\xe9fi pour les grands mod\xe8les de langage est les hallucinations, ce qui signifie qu'ils inventent parfois des choses ou se trompent manifestement et ils le seront.  confiants d'avoir tort aussi, ils diront les choses avec la plus grande confiance, mais ils auront compl\xe8tement tort, regardez cet exemple combien de lettres il y a dans la cha\xeene, puis nous lui donnons une cha\xeene de caract\xe8res al\xe9atoire et la r\xe9ponse est que la cha\xeene a m\xeame 16 lettres  bien qu'il ne contienne que 15 lettres, un autre probl\xe8me est que les grands mod\xe8les de langage sont extr\xeamement gourmands en mat\xe9riel, ils co\xfbtent tr\xe8s cher \xe0 former et \xe0 affiner car cela prend beaucoup de puissance de traitement pour le faire et il y a beaucoup d'\xe9thique \xe0 prendre en compte aussi  des entreprises d'IA d\xe9clarent qu'elles n'entra\xeenent pas leurs mod\xe8les sur du mat\xe9riel prot\xe9g\xe9 par le droit d'auteur, mais cela s'est av\xe9r\xe9 faux. Actuellement, de nombreuses poursuites sont en cours devant les tribunaux \xe0 ce sujet. Parlons ensuite des applications r\xe9elles des grands mod\xe8les de langage. Pourquoi sont-ils  ils sont si pr\xe9cieux, pourquoi on en parle tant et pourquoi transforment-ils le monde sous nos yeux de grands mod\xe8les de langage peuvent \xeatre utilis\xe9s pour une grande vari\xe9t\xe9 de t\xe2ches, pas seulement des chatbots, ils peuvent \xeatre utilis\xe9s pour la traduction linguistique, ils peuvent \xeatre utilis\xe9s pour  codage, ils peuvent \xeatre utilis\xe9s comme assistants de programmation, ils peuvent \xeatre utilis\xe9s pour r\xe9sumer les questions, r\xe9pondre \xe0 la r\xe9daction d'essais, traduire et m\xeame cr\xe9er des images et des vid\xe9os. Fondamentalement, tout type de probl\xe8me de pens\xe9e qu'un humain peut r\xe9soudre avec un ordinateur. De grands mod\xe8les de langage peuvent probablement aussi le faire, sinon aujourd'hui. bient\xf4t dans le futur, parlons maintenant des avanc\xe9es et de la recherche actuelles. Actuellement, on parle beaucoup de distillation des connaissances, ce qui signifie essentiellement transf\xe9rer des connaissances cl\xe9s de tr\xe8s grands mod\xe8les de pointe vers des mod\xe8les plus petits et plus efficaces. Pensez- y comme un professeur condensant des d\xe9cennies d'exp\xe9rience dans un  manuel jusqu'\xe0 quelque chose que les \xe9tudiants peuvent comprendre, ce qui permet \xe0 des mod\xe8les de langage plus petits de b\xe9n\xe9ficier des connaissances acquises gr\xe2ce \xe0 ces grands mod\xe8les de langage, tout en fonctionnant toujours de mani\xe8re tr\xe8s efficace sur le mat\xe9riel grand public de tous les jours. Cela rend les grands mod\xe8les de langage plus accessibles et plus pratiques \xe0 ex\xe9cuter, m\xeame sur  sur les t\xe9l\xe9phones portables ou autres appareils finaux, il y a \xe9galement eu beaucoup de recherches et l'accent a \xe9t\xe9 mis sur rag, qui est une g\xe9n\xe9ration augment\xe9e de r\xe9cup\xe9ration, ce qui signifie essentiellement que vous donnez \xe0 de grands mod\xe8les de langage la possibilit\xe9 de rechercher des informations en dehors des donn\xe9es sur lesquelles ils ont \xe9t\xe9 form\xe9s.  en utilisant les bases de donn\xe9es Vector de la m\xeame mani\xe8re que les grands mod\xe8les de langage sont form\xe9s, mais vous \xeates capable de stocker d'\xe9normes quantit\xe9s de donn\xe9es suppl\xe9mentaires qui peuvent \xeatre interrog\xe9es par ce grand mod\xe8le de langage, parlons maintenant des consid\xe9rations \xe9thiques et il y a beaucoup de choses \xe0 penser ici et moi  J'aborde simplement certains des sujets majeurs dont nous avons d\xe9j\xe0 parl\xe9, \xe0 savoir que les mod\xe8les sont form\xe9s sur du mat\xe9riel potentiellement prot\xe9g\xe9 par le droit d'auteur et si tel est le cas, l'utilisation \xe9quitable ne sera probablement pas la prochaine \xe9tape. Ces mod\xe8les peuvent et seront utilis\xe9s pour des actes nuisibles, il n'y a pas moyen de l' \xe9viter.  les grands mod\xe8les de langage peuvent \xeatre utilis\xe9s pour arnaquer d'autres personnes afin de cr\xe9er des campagnes massives de d\xe9sinformation et de d\xe9sinformation, y compris de fausses images, de faux textes, de fausses opinions et il est presque certain que l'ensemble des cols blancs sera perturb\xe9 par de grands mod\xe8les de langage, comme je l'ai mentionn\xe9, tout ce que n'importe qui peut faire devant  d'un ordinateur est probablement quelque chose que l'IA peut aussi faire, donc avocats, \xe9crivains, programmeurs, il y a tellement de professions diff\xe9rentes qui vont \xeatre compl\xe8tement perturb\xe9es par l' intelligence artificielle et puis finalement AGI, que se passe-t-il lorsque l'IA devient si intelligente et peut-\xeatre m\xeame commence \xe0 penser par elle-m\xeame  c'est l\xe0 que nous devons avoir quelque chose appel\xe9 alignement, ce qui signifie que l'IA est align\xe9e sur les m\xeames incitations et r\xe9sultats que les humains. Parlons donc pour la derni\xe8re fois de ce qui se passe sur The Cutting Edge et dans un avenir imm\xe9diat, il existe un certain nombre de fa\xe7ons dont les grands mod\xe8les de langage peuvent  \xeatre am\xe9lior\xe9s, ils peuvent d'abord v\xe9rifier eux-m\xeames les faits avec les informations recueillies sur le Web, mais vous pouvez \xe9videmment voir les d\xe9fauts inh\xe9rents \xe0 cela, puis nous avons \xe9galement abord\xe9 le m\xe9lange d'experts qui est une nouvelle technologie incroyable qui permet \xe0 plusieurs mod\xe8les d'\xeatre en quelque sorte fusionn\xe9s, tout va bien. Adaptez-vous pour \xeatre des experts dans certains domaines, puis lorsque l'invite r\xe9elle arrive, il choisit lequel de ces experts utiliser, ce sont donc d'\xe9normes mod\xe8les qui fonctionnent vraiment tr\xe8s efficacement et puis il y a beaucoup de travail sur la multimodalit\xe9, donc en prenant la contribution de la voix \xe0 partir des images  \xe0 partir de la vid\xe9o, de toutes les sources d'entr\xe9e possibles et d'avoir une seule sortie \xe0 partir de celle-ci, beaucoup de travail est \xe9galement fait pour am\xe9liorer la capacit\xe9 de raisonnement, faire r\xe9fl\xe9chir les mod\xe8les lentement est une nouvelle tendance que j'ai vue dans des articles comme orca \xe9galement, qui force simplement un grand mod\xe8le de langage pour r\xe9fl\xe9chir aux probl\xe8mes \xe9tape par \xe9tape plut\xf4t que d'essayer de sauter imm\xe9diatement \xe0 la conclusion finale, puis \xe9galement \xe0 des tailles de contexte plus grandes si vous souhaitez qu'un grand mod\xe8le de langage traite une \xe9norme quantit\xe9 de donn\xe9es, il doit avoir une tr\xe8s grande fen\xeatre de contexte et un La fen\xeatre contextuelle indique la quantit\xe9 d'informations que vous pouvez donner \xe0 une invite pour obtenir le r\xe9sultat. Une fa\xe7on d'y parvenir est de donner de la m\xe9moire \xe0 de grands mod\xe8les de langage avec des projets comme mgpt sur lesquels j'ai fait une vid\xe9o et je vais le laisser dans la description ci-dessous.  et cela signifie simplement donner aux mod\xe8les une m\xe9moire externe \xe0 partir de cet ensemble de donn\xe9es de base sur lequel ils ont \xe9t\xe9 form\xe9s, donc c'est tout pour aujourd'hui si vous avez aim\xe9 cette vid\xe9o, pensez \xe0 donner un like et abonnez-vous, consultez AI Camp, je laisserai toutes les informations dans la description ci-dessous  et bien s\xfbr, regardez l'une de mes autres vid\xe9os sur l'IA si vous voulez en savoir encore plus, je vous verrai dans la prochaine\""})}function c(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>r});var t=n(6540);const a={},i=t.createContext(a);function o(e){const s=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(i.Provider,{value:s},e.children)}}}]);