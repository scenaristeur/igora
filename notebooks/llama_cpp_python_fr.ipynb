{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8ll0B3gHDzC8v/ybo3cIt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scenaristeur/igora/blob/main/notebooks/llama_cpp_python_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Téléchargement d'un modèle de langage (dolphin) au format .GGUF depuis HuggingFace / TheBloke https://huggingface.co/TheBloke dans le dossier ./models"
      ],
      "metadata": {
        "id": "-1FrZyiUwr5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVEbYUTdwPWq"
      },
      "outputs": [],
      "source": [
        "!mkdir models\n",
        "!wget -O ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pvVfiHcYyqW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation de llama-cpp-python version CPU"
      ],
      "metadata": {
        "id": "DYH7A-JMyV2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
      ],
      "metadata": {
        "id": "0d4-zT3dyh6-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On peut maintenant poser une question au modèle de langage en suivant la démo\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "Wc6J5Nl-y8-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Nomme les planètes du système solaire.\", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse\", reponse)"
      ],
      "metadata": {
        "id": "mLKpbPBYzL-G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}