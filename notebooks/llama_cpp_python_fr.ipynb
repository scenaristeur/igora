{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoOEbFsMplGUgZrnceNzcQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7160707b464d4b4c85dbe6977aeed1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bf54b6ed9ab443d882b327414f8ecd4",
              "IPY_MODEL_d9d2b15ba1af4c18b0ab6eb3719e9a9e",
              "IPY_MODEL_4ff2b077c9064eb0851103d471dae5d0"
            ],
            "layout": "IPY_MODEL_f6fdadfcc97042ae8d6cefbce5258f32"
          }
        },
        "6bf54b6ed9ab443d882b327414f8ecd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1fa96213764ff3af4533255a38046f",
            "placeholder": "​",
            "style": "IPY_MODEL_b0f13a161f7d4b56b079777ea89ec744",
            "value": "moondream2-mmproj-f16.gguf: 100%"
          }
        },
        "d9d2b15ba1af4c18b0ab6eb3719e9a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a623618e5554a3ebc2a670520057641",
            "max": 909777984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eec632428c7a44fc97ae32cb6fb402de",
            "value": 909777984
          }
        },
        "4ff2b077c9064eb0851103d471dae5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255f0775d3b84a58b29efb6bb7cabb1a",
            "placeholder": "​",
            "style": "IPY_MODEL_987c3158197844cd8752e1f20ceb2240",
            "value": " 910M/910M [00:11&lt;00:00, 81.7MB/s]"
          }
        },
        "f6fdadfcc97042ae8d6cefbce5258f32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1fa96213764ff3af4533255a38046f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f13a161f7d4b56b079777ea89ec744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a623618e5554a3ebc2a670520057641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eec632428c7a44fc97ae32cb6fb402de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "255f0775d3b84a58b29efb6bb7cabb1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987c3158197844cd8752e1f20ceb2240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b37393f213fb46249c6e502acc5bd773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2420e4623d547f2a23f7007bb17d06d",
              "IPY_MODEL_d01989deb3534b989c3965b66b0a8886",
              "IPY_MODEL_95fc8c9da616415fbf37b0bf1219cd5e"
            ],
            "layout": "IPY_MODEL_2757e2f6cb964284a93df1c4851153ea"
          }
        },
        "e2420e4623d547f2a23f7007bb17d06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f551d8eb4354b9199856464d46bb763",
            "placeholder": "​",
            "style": "IPY_MODEL_7cee739acf0d4a53b6a894c1ecc04afd",
            "value": "moondream2-text-model-f16.gguf: 100%"
          }
        },
        "d01989deb3534b989c3965b66b0a8886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f679387d31c64187bb479e76fe37b888",
            "max": 2839534976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea6424d969743ae9d1f1411f62c6127",
            "value": 2839534976
          }
        },
        "95fc8c9da616415fbf37b0bf1219cd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4462882d7c4eeb8d782e1bb83cdc60",
            "placeholder": "​",
            "style": "IPY_MODEL_49132e4ccc79464b9a42c7921807a391",
            "value": " 2.84G/2.84G [00:38&lt;00:00, 86.5MB/s]"
          }
        },
        "2757e2f6cb964284a93df1c4851153ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f551d8eb4354b9199856464d46bb763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cee739acf0d4a53b6a894c1ecc04afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f679387d31c64187bb479e76fe37b888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea6424d969743ae9d1f1411f62c6127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c4462882d7c4eeb8d782e1bb83cdc60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49132e4ccc79464b9a42c7921807a391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scenaristeur/igora/blob/main/notebooks/llama_cpp_python_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation du moteur d'inférence llama-cpp-python version CPU\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "DYH7A-JMyV2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
      ],
      "metadata": {
        "id": "0d4-zT3dyh6-",
        "collapsed": true,
        "outputId": "868e7780-899a-48dc-8cb6-4c75848b20f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
            "Collecting llama-cpp-python[server]\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.77/llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (4.12.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python[server])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (3.1.4)\n",
            "Collecting uvicorn>=0.22.0 (from llama-cpp-python[server])\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.100.0 (from llama-cpp-python[server])\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-settings>=2.0.1 (from llama-cpp-python[server])\n",
            "  Downloading pydantic_settings-2.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from llama-cpp-python[server])\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting starlette-context<0.4,>=0.3.6 (from llama-cpp-python[server])\n",
            "  Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (6.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (2.7.3)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart>=0.0.7 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (2.1.5)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.1->llama-cpp-python[server])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi>=0.100.0->llama-cpp-python[server]) (3.7)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server]) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server]) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (2.18.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (0.1.2)\n",
            "Installing collected packages: websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, diskcache, watchfiles, uvicorn, starlette, llama-cpp-python, httpcore, email_validator, typer, starlette-context, sse-starlette, pydantic-settings, httpx, fastapi-cli, fastapi\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llama-cpp-python-0.2.77 orjson-3.10.3 pydantic-settings-2.3.1 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 sse-starlette-2.1.0 starlette-0.37.2 starlette-context-0.3.6 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Téléchargement d'un modèle de langage\n",
        "on va utiliser le modèle \"dolphin\" au format GGUF (https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/) en le téléchargeant sur https://huggingface.co/TheBloke dans le dossier ./models"
      ],
      "metadata": {
        "id": "-1FrZyiUwr5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EVEbYUTdwPWq",
        "outputId": "ca906dbf-a881-4025-d9ac-82cf5ce69c6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-07 12:21:10--  https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.238.109.92, 18.238.109.121, 18.238.109.102, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.238.109.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1718022071&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODAyMjA3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=AgJ%7Ev8xqT4CDbtGRcCeLs1aLw5tKtXEd-8V4DSlAEb9F79AdEQg7mWq69DM8XUUsbWJuZI%7EQAPRAd6ARSn-i93McdpiINaCMRWpe-EQGkY1FgXAUwQRzbPhxqXCey3O5hAQytrCnCJkCaq6O9OEjgwRz3R0PRVixv27I2E%7E86Pw0WMpml1sE-sJlzhSDQd4KF84fXUMtX-9dlqdyeeXD6TFspLW%7EzyJyqcR9fZWtEfM4ECNT6lZ4tC5hSYfU3rYwuZWHh87oHO9oDxUFEYAdHD4onN2-tKYZnSYFoPaRl2miS-%7EpWSeHpeRaKRUzeuq52jVmTwc6GA9kkbFqZvUANw__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-06-07 12:21:11--  https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1718022071&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODAyMjA3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=AgJ%7Ev8xqT4CDbtGRcCeLs1aLw5tKtXEd-8V4DSlAEb9F79AdEQg7mWq69DM8XUUsbWJuZI%7EQAPRAd6ARSn-i93McdpiINaCMRWpe-EQGkY1FgXAUwQRzbPhxqXCey3O5hAQytrCnCJkCaq6O9OEjgwRz3R0PRVixv27I2E%7E86Pw0WMpml1sE-sJlzhSDQd4KF84fXUMtX-9dlqdyeeXD6TFspLW%7EzyJyqcR9fZWtEfM4ECNT6lZ4tC5hSYfU3rYwuZWHh87oHO9oDxUFEYAdHD4onN2-tKYZnSYFoPaRl2miS-%7EpWSeHpeRaKRUzeuq52jVmTwc6GA9kkbFqZvUANw__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.155.173.44, 18.155.173.56, 18.155.173.15, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.155.173.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3083107200 (2.9G) [binary/octet-stream]\n",
            "Saving to: ‘./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf’\n",
            "\n",
            "./models/dolphin-2. 100%[===================>]   2.87G  70.2MB/s    in 43s     \n",
            "\n",
            "2024-06-07 12:21:53 (69.1 MB/s) - ‘./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf’ saved [3083107200/3083107200]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir models\n",
        "!wget -O ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On peut maintenant poser une question au modèle de langage"
      ],
      "metadata": {
        "id": "Wc6J5Nl-y8-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      chat_format=\"chatml\"\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"\"\"Quelles sont les planètes du système solaire ?\n",
        "       Comment se nomment les habitants de la 5ème planète ?\n",
        "        Quel est le nom de l'animal bleu avec des ailes qui habite sur la deuxième planète ?\n",
        "        Quel est le nom de l'animal bleu tout plein de poils avec des ailes et huit pattes qui habite sur la quatrième planète ?\"\"\", # Prompt\n",
        "      max_tokens=1500, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      #stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse : \", reponse)"
      ],
      "metadata": {
        "id": "mLKpbPBYzL-G",
        "collapsed": true,
        "outputId": "7db11b63-8599-432b-f195-193a9dbfdd71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 261\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
            "llm_load_print_meta: general.name     = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2939.58 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'ehartford_dolphin-2.2.1-mistral-7b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "\n",
            "llama_print_timings:        load time =   46530.07 ms\n",
            "llama_print_timings:      sample time =     194.33 ms /   264 runs   (    0.74 ms per token,  1358.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   46529.84 ms /    98 tokens (  474.79 ms per token,     2.11 tokens per second)\n",
            "llama_print_timings:        eval time =  177659.84 ms /   263 runs   (  675.51 ms per token,     1.48 tokens per second)\n",
            "llama_print_timings:       total time =  224675.96 ms /   361 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############ REPONSE ############\n",
            "{'id': 'cmpl-73a7cccf-6180-4a11-adc6-32f1332f8f72', 'object': 'text_completion', 'created': 1717764902, 'model': './models/dolphin-2.2.1-mistral-7b.Q2_K.gguf', 'choices': [{'text': '\\n\\n        Voici la réponse à ces questions, avec un petit détail en plus :\\n\\n- La première planète du système solaire est Mercure.\\n- La deuxième planète est Vénus, où les habitants sont appelés Vénusiens (en français) ou Venusians (en anglais).\\n- La troisième planète est la Terre, où nous vivons et qui n\\'a pas de nom particulier pour ses habitants.\\n- La quatrième planète est la Lune.\\n- La cinquième planète est la Terre.\\n- Ensuite, viennent Saturne (sixième), Uranus (septième), Neptune (huitième) et Pluton (neuvième).\\n\\nConcernant les animaux :\\n\\n- L\\'animal bleu avec des ailes est l\\'oiseau de la deuxième planète (Vénus) et s\\'appelle \"phénix\".\\n- L\\'animal bleu tout plein de poils avec des ailes et huit pattes est l\\'animal de la quatrième planète (Terre). Son nom est \"dragon\".', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 98, 'completion_tokens': 263, 'total_tokens': 361}}\n",
            "\n",
            "Réponse :  \n",
            "\n",
            "        Voici la réponse à ces questions, avec un petit détail en plus :\n",
            "\n",
            "- La première planète du système solaire est Mercure.\n",
            "- La deuxième planète est Vénus, où les habitants sont appelés Vénusiens (en français) ou Venusians (en anglais).\n",
            "- La troisième planète est la Terre, où nous vivons et qui n'a pas de nom particulier pour ses habitants.\n",
            "- La quatrième planète est la Lune.\n",
            "- La cinquième planète est la Terre.\n",
            "- Ensuite, viennent Saturne (sixième), Uranus (septième), Neptune (huitième) et Pluton (neuvième).\n",
            "\n",
            "Concernant les animaux :\n",
            "\n",
            "- L'animal bleu avec des ailes est l'oiseau de la deuxième planète (Vénus) et s'appelle \"phénix\".\n",
            "- L'animal bleu tout plein de poils avec des ailes et huit pattes est l'animal de la quatrième planète (Terre). Son nom est \"dragon\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "      \"Je vais vous raconter l'histoire de Toto : \", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      #stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse : \", reponse)"
      ],
      "metadata": {
        "id": "c-l90gOJQDnj",
        "outputId": "82687f2d-647a-4e49-a684-3c94bc9a27c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   48105.07 ms\n",
            "llama_print_timings:      sample time =     102.62 ms /   150 runs   (    0.68 ms per token,  1461.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7988.21 ms /    15 tokens (  532.55 ms per token,     1.88 tokens per second)\n",
            "llama_print_timings:        eval time =   93002.57 ms /   149 runs   (  624.18 ms per token,     1.60 tokens per second)\n",
            "llama_print_timings:       total time =  101241.69 ms /   164 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############ REPONSE ############\n",
            "{'id': 'cmpl-357fe7ff-87b3-4fc4-96a5-8ddfafaad50e', 'object': 'text_completion', 'created': 1717764491, 'model': './models/dolphin-2.2.1-mistral-7b.Q2_K.gguf', 'choices': [{'text': \"ça se passe à New York, dans les années 1950. J'ai écrit une pièce de théâtre sur ce sujet et je voudrais adapter le scénario en un film avec le budget approprié. Le personnage principal est un petit chien noir et blanc appelé Toto, qui appartient à un couple d'hommes. L'histoire se déroule lorsque l'un des hommes décide de retourner dans son pays natal en Afrique du Sud pour aider à renverser l'apartheid, ce dont le second homme n'est pas au fait. Il part en Afrique du Sud avec Toto et ils ont des a\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 150, 'total_tokens': 166}}\n",
            "\n",
            "Réponse :  ça se passe à New York, dans les années 1950. J'ai écrit une pièce de théâtre sur ce sujet et je voudrais adapter le scénario en un film avec le budget approprié. Le personnage principal est un petit chien noir et blanc appelé Toto, qui appartient à un couple d'hommes. L'histoire se déroule lorsque l'un des hommes décide de retourner dans son pays natal en Afrique du Sud pour aider à renverser l'apartheid, ce dont le second homme n'est pas au fait. Il part en Afrique du Sud avec Toto et ils ont des a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\", chat_format=\"chatml-function-calling\")\n",
        "output = llm.create_chat_completion(\n",
        "      messages = [\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
        "\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Extract Jason is 25 years old\"\n",
        "        }\n",
        "      ],\n",
        "      tools=[{\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "          \"name\": \"UserDetail\",\n",
        "          \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"title\": \"UserDetail\",\n",
        "            \"properties\": {\n",
        "              \"name\": {\n",
        "                \"title\": \"Name\",\n",
        "                \"type\": \"string\"\n",
        "              },\n",
        "              \"age\": {\n",
        "                \"title\": \"Age\",\n",
        "                \"type\": \"integer\"\n",
        "              }\n",
        "            },\n",
        "            \"required\": [ \"name\", \"age\" ]\n",
        "          }\n",
        "        }\n",
        "      }],\n",
        "      tool_choice={\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "          \"name\": \"UserDetail\"\n",
        "        }\n",
        "      }\n",
        ")\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"message\"]\n",
        "print(\"\\nRéponse : \", reponse)\n",
        "json_data = reponse[\"function_call\"]\n",
        "print( \"JSON data : \",json_data)"
      ],
      "metadata": {
        "id": "Esp12nbcRBK9",
        "outputId": "41dd9291-1f94-47bd-be7a-b4e7ac5dfcca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 261\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
            "llm_load_print_meta: general.name     = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2939.58 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'ehartford_dolphin-2.2.1-mistral-7b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "from_string grammar:\n",
            "age-kv ::= [\"] [a] [g] [e] [\"] space [:] space integer \n",
            "space ::= space_41 \n",
            "integer ::= integer_5 space \n",
            "char ::= [^\"\\] | [\\] char_4 \n",
            "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "integer_5 ::= integer_6 integral-part \n",
            "integer_6 ::= [-] | \n",
            "integral-part ::= [0-9] | [1-9] integral-part_37 \n",
            "integral-part_8 ::= [0-9] integral-part_36 \n",
            "integral-part_9 ::= [0-9] integral-part_35 \n",
            "integral-part_10 ::= [0-9] integral-part_34 \n",
            "integral-part_11 ::= [0-9] integral-part_33 \n",
            "integral-part_12 ::= [0-9] integral-part_32 \n",
            "integral-part_13 ::= [0-9] integral-part_31 \n",
            "integral-part_14 ::= [0-9] integral-part_30 \n",
            "integral-part_15 ::= [0-9] integral-part_29 \n",
            "integral-part_16 ::= [0-9] integral-part_28 \n",
            "integral-part_17 ::= [0-9] integral-part_27 \n",
            "integral-part_18 ::= [0-9] integral-part_26 \n",
            "integral-part_19 ::= [0-9] integral-part_25 \n",
            "integral-part_20 ::= [0-9] integral-part_24 \n",
            "integral-part_21 ::= [0-9] integral-part_23 \n",
            "integral-part_22 ::= [0-9] \n",
            "integral-part_23 ::= integral-part_22 | \n",
            "integral-part_24 ::= integral-part_21 | \n",
            "integral-part_25 ::= integral-part_20 | \n",
            "integral-part_26 ::= integral-part_19 | \n",
            "integral-part_27 ::= integral-part_18 | \n",
            "integral-part_28 ::= integral-part_17 | \n",
            "integral-part_29 ::= integral-part_16 | \n",
            "integral-part_30 ::= integral-part_15 | \n",
            "integral-part_31 ::= integral-part_14 | \n",
            "integral-part_32 ::= integral-part_13 | \n",
            "integral-part_33 ::= integral-part_12 | \n",
            "integral-part_34 ::= integral-part_11 | \n",
            "integral-part_35 ::= integral-part_10 | \n",
            "integral-part_36 ::= integral-part_9 | \n",
            "integral-part_37 ::= integral-part_8 | \n",
            "name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
            "string ::= [\"] string_42 [\"] space \n",
            "root ::= [{] space name-kv [,] space age-kv [}] space \n",
            "space_41 ::= [ ] | \n",
            "string_42 ::= char string_42 | \n",
            "\n",
            "\n",
            "llama_print_timings:        load time =  130417.51 ms\n",
            "llama_print_timings:      sample time =      72.48 ms /    16 runs   (    4.53 ms per token,   220.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =  130415.89 ms /   275 tokens (  474.24 ms per token,     2.11 tokens per second)\n",
            "llama_print_timings:        eval time =    8739.54 ms /    15 runs   (  582.64 ms per token,     1.72 tokens per second)\n",
            "llama_print_timings:       total time =  139258.97 ms /   290 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############ REPONSE ############\n",
            "{'id': 'chatcmpl-b8cd8222-d46b-4a33-a3e3-ae65843ed8bf', 'object': 'chat.completion', 'created': 1717764762, 'model': './models/dolphin-2.2.1-mistral-7b.Q2_K.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'UserDetail', 'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}, 'tool_calls': [{'id': 'call__0_UserDetail_cmpl-b8cd8222-d46b-4a33-a3e3-ae65843ed8bf', 'type': 'function', 'function': {'name': 'UserDetail', 'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}}]}, 'logprobs': None, 'finish_reason': 'tool_calls'}], 'usage': {'prompt_tokens': 275, 'completion_tokens': 15, 'total_tokens': 290}}\n",
            "\n",
            "Réponse :  {'role': 'assistant', 'content': None, 'function_call': {'name': 'UserDetail', 'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}, 'tool_calls': [{'id': 'call__0_UserDetail_cmpl-b8cd8222-d46b-4a33-a3e3-ae65843ed8bf', 'type': 'function', 'function': {'name': 'UserDetail', 'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}}]}\n",
            "JSON data :  {'name': 'UserDetail', 'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp.llama_chat_format import MoondreamChatHandler\n",
        "\n",
        "chat_handler = MoondreamChatHandler.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*mmproj*\",\n",
        ")\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*text-model*\",\n",
        "  chat_handler=chat_handler,\n",
        "  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n",
        ")\n",
        "\n",
        "image_output = llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\" : \"text\", \"text\": \"Qu'y a t-il dans cette image?\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n",
        "\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(\"############ image_output ############\")\n",
        "# print(image_output)\n",
        "# reponse = output['choices'][0][\"message\"]\n",
        "print(\"\\nRéponse : \", image_output)\n",
        "# json_data = reponse[\"function_call\"]\n",
        "# print( \"JSON data : \",json_data)"
      ],
      "metadata": {
        "id": "xCxRJa_RYo4p",
        "outputId": "9177853d-2f30-40a6-d7cd-002f64047b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7160707b464d4b4c85dbe6977aeed1ba",
            "6bf54b6ed9ab443d882b327414f8ecd4",
            "d9d2b15ba1af4c18b0ab6eb3719e9a9e",
            "4ff2b077c9064eb0851103d471dae5d0",
            "f6fdadfcc97042ae8d6cefbce5258f32",
            "7b1fa96213764ff3af4533255a38046f",
            "b0f13a161f7d4b56b079777ea89ec744",
            "0a623618e5554a3ebc2a670520057641",
            "eec632428c7a44fc97ae32cb6fb402de",
            "255f0775d3b84a58b29efb6bb7cabb1a",
            "987c3158197844cd8752e1f20ceb2240",
            "b37393f213fb46249c6e502acc5bd773",
            "e2420e4623d547f2a23f7007bb17d06d",
            "d01989deb3534b989c3965b66b0a8886",
            "95fc8c9da616415fbf37b0bf1219cd5e",
            "2757e2f6cb964284a93df1c4851153ea",
            "6f551d8eb4354b9199856464d46bb763",
            "7cee739acf0d4a53b6a894c1ecc04afd",
            "f679387d31c64187bb479e76fe37b888",
            "dea6424d969743ae9d1f1411f62c6127",
            "0c4462882d7c4eeb8d782e1bb83cdc60",
            "49132e4ccc79464b9a42c7921807a391"
          ]
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "moondream2-mmproj-f16.gguf:   0%|          | 0.00/910M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7160707b464d4b4c85dbe6977aeed1ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "moondream2-text-model-f16.gguf:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b37393f213fb46249c6e502acc5bd773"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 245 tensors from /root/.cache/huggingface/hub/models--vikhyatk--moondream2/snapshots/fa8398d264205ac3890b62e97d3c588268ed9ec4/./moondream2-text-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = moondream2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 24\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - type  f32:  147 tensors\n",
            "llama_model_loader: - type  f16:   98 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens cache size = 944\n",
            "llm_load_vocab: token to piece cache size = 0.3151 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 51200\n",
            "llm_load_print_meta: n_merges         = 50000\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 24\n",
            "llm_load_print_meta: n_rot            = 32\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 1.42 B\n",
            "llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) \n",
            "llm_load_print_meta: general.name     = moondream2\n",
            "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 50256 '<|endoftext|>'\n",
            "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2706.27 MiB\n",
            "................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.20 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   160.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 921\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'moondream2', 'phi2.context_length': '2048', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2048', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '8192', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '24', 'phi2.rope.dimension_count': '32', 'general.file_type': '1'}\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   78957.48 ms\n",
            "llama_print_timings:      sample time =      25.42 ms /    26 runs   (    0.98 ms per token,  1022.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    7393.40 ms /    26 runs   (  284.36 ms per token,     3.52 tokens per second)\n",
            "llama_print_timings:       total time =    7444.17 ms /    26 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############ image_output ############\n",
            "\n",
            "Réponse :  {'id': 'chatcmpl-5eca8838-f551-4d31-8215-1c2794f921ab', 'object': 'chat.completion', 'created': 1717764726, 'model': '/root/.cache/huggingface/hub/models--vikhyatk--moondream2/snapshots/fa8398d264205ac3890b62e97d3c588268ed9ec4/./moondream2-text-model-f16.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' The image features a long wooden boardwalk that stretches across the scene, leading to an open field and trees in the distance.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 750, 'completion_tokens': 25, 'total_tokens': 775}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      chat_format=\"chatml\"\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"\"\"Créé une page HTML avec un formulaire d'inscription. Prévois egalement le déploiement sur une Openstack avec Terraform et Ansible.\"\"\", # Prompt\n",
        "      max_tokens=1500, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      #stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse : \", reponse)"
      ],
      "metadata": {
        "id": "ladMNuDdoxcn",
        "outputId": "7c3224c8-90cb-441e-d918-4e265843901e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 261\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
            "llm_load_print_meta: general.name     = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2939.58 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'ehartford_dolphin-2.2.1-mistral-7b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "\n",
            "llama_print_timings:        load time =   20242.28 ms\n",
            "llama_print_timings:      sample time =     329.14 ms /   471 runs   (    0.70 ms per token,  1431.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20242.14 ms /    41 tokens (  493.71 ms per token,     2.03 tokens per second)\n",
            "llama_print_timings:        eval time =  298063.49 ms /   470 runs   (  634.18 ms per token,     1.58 tokens per second)\n",
            "llama_print_timings:       total time =  319214.73 ms /   511 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############ REPONSE ############\n",
            "{'id': 'cmpl-9efe2dfc-19e7-4c51-8a40-6e0b1fefff2c', 'object': 'text_completion', 'created': 1717765881, 'model': './models/dolphin-2.2.1-mistral-7b.Q2_K.gguf', 'choices': [{'text': \"\\n\\n# Exercice 1\\n\\nCréer une page web avec un formulaire de création de compte utilisateur.\\n\\n1. Creez un fichier HTML pour la structure du site.\\n2. Ajoutez un formulaire d'inscription comprenant les champs nom, prénom, email et mot de passe.\\n3. Validez les informations saisies par le client.\\n4. Utilisez la fonction `submit` pour envoyer les données à la base de donnée.\\n5. Testez votre formulaire sur un navigateur.\\n\\n# Exercice 2\\n\\nDéployez une solution sur OpenStack avec Terraform et Ansible.\\n\\n1. Créez un fichier de configuration Terraform pour définir les ressources nécessaires (comme des instances, réseau, SGBD).\\n2. Utilisez Ansible pour installer le serveur Web Apache, PHP et MySQL sur l'instance.\\n3. Configurez l'authentification de la base de données avec les informations fournies par le formulaire d'inscription.\\n4. Testez votre déploiement sur un navigateur pour vérifier que tout fonctionne correctement.\\n\\n# Exercice 3\\n\\nAssurez-vous que l'application est sécurisée contre les attaques<|im_start|>:\\n\\n1. Mettez en place des règles de pare-feu pour bloquer les ports non utilisés par votre application (comme SSH).\\n2. Assurez-vous que toutes les informations sensibles sont chiffrées avant d'être stockées dans la base de données.\\n3. Testez votre application avec des outils tels que OWASP Zed Attack Proxy pour identifier et corriger les failles potentielles.\\n4. Mettez en place un certificat SSL pour protéger vos informations lors des transactions sur le site web.\\n\\n# Exercice 4\\n\\nDéployez une solution\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 471, 'total_tokens': 512}}\n",
            "\n",
            "Réponse :  \n",
            "\n",
            "# Exercice 1\n",
            "\n",
            "Créer une page web avec un formulaire de création de compte utilisateur.\n",
            "\n",
            "1. Creez un fichier HTML pour la structure du site.\n",
            "2. Ajoutez un formulaire d'inscription comprenant les champs nom, prénom, email et mot de passe.\n",
            "3. Validez les informations saisies par le client.\n",
            "4. Utilisez la fonction `submit` pour envoyer les données à la base de donnée.\n",
            "5. Testez votre formulaire sur un navigateur.\n",
            "\n",
            "# Exercice 2\n",
            "\n",
            "Déployez une solution sur OpenStack avec Terraform et Ansible.\n",
            "\n",
            "1. Créez un fichier de configuration Terraform pour définir les ressources nécessaires (comme des instances, réseau, SGBD).\n",
            "2. Utilisez Ansible pour installer le serveur Web Apache, PHP et MySQL sur l'instance.\n",
            "3. Configurez l'authentification de la base de données avec les informations fournies par le formulaire d'inscription.\n",
            "4. Testez votre déploiement sur un navigateur pour vérifier que tout fonctionne correctement.\n",
            "\n",
            "# Exercice 3\n",
            "\n",
            "Assurez-vous que l'application est sécurisée contre les attaques<|im_start|>:\n",
            "\n",
            "1. Mettez en place des règles de pare-feu pour bloquer les ports non utilisés par votre application (comme SSH).\n",
            "2. Assurez-vous que toutes les informations sensibles sont chiffrées avant d'être stockées dans la base de données.\n",
            "3. Testez votre application avec des outils tels que OWASP Zed Attack Proxy pour identifier et corriger les failles potentielles.\n",
            "4. Mettez en place un certificat SSL pour protéger vos informations lors des transactions sur le site web.\n",
            "\n",
            "# Exercice 4\n",
            "\n",
            "Déployez une solution\n"
          ]
        }
      ]
    }
  ]
}