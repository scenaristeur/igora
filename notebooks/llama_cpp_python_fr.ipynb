{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/sXtzB7RNZ1sWuDAgwuYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scenaristeur/igora/blob/main/notebooks/llama_cpp_python_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation du moteur d'inférence llama-cpp-python version CPU\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "DYH7A-JMyV2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
      ],
      "metadata": {
        "id": "0d4-zT3dyh6-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Téléchargement d'un modèle de langage\n",
        "on va utiliser le modèle \"dolphin\" au format GGUF (https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/) en le téléchargeant sur https://huggingface.co/TheBloke dans le dossier ./models"
      ],
      "metadata": {
        "id": "-1FrZyiUwr5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVEbYUTdwPWq"
      },
      "outputs": [],
      "source": [
        "!mkdir models\n",
        "!wget -O ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On peut maintenant poser une question au modèle de langage"
      ],
      "metadata": {
        "id": "Wc6J5Nl-y8-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      chat_format=\"chatml\"\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      #stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse : \", reponse)"
      ],
      "metadata": {
        "id": "mLKpbPBYzL-G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "      \"Je vais vous raconter l'histoire de Toto : \", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      #stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse : \", reponse)"
      ],
      "metadata": {
        "id": "c-l90gOJQDnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\", chat_format=\"chatml-function-calling\")\n",
        "output = llm.create_chat_completion(\n",
        "      messages = [\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
        "\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Extract Jason is 25 years old\"\n",
        "        }\n",
        "      ],\n",
        "      tools=[{\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "          \"name\": \"UserDetail\",\n",
        "          \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"title\": \"UserDetail\",\n",
        "            \"properties\": {\n",
        "              \"name\": {\n",
        "                \"title\": \"Name\",\n",
        "                \"type\": \"string\"\n",
        "              },\n",
        "              \"age\": {\n",
        "                \"title\": \"Age\",\n",
        "                \"type\": \"integer\"\n",
        "              }\n",
        "            },\n",
        "            \"required\": [ \"name\", \"age\" ]\n",
        "          }\n",
        "        }\n",
        "      }],\n",
        "      tool_choice={\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "          \"name\": \"UserDetail\"\n",
        "        }\n",
        "      }\n",
        ")\n",
        "\n",
        "print(\"############ REPONSE ############\")\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"message\"]\n",
        "print(\"\\nRéponse : \", reponse)\n",
        "json_data = reponse[\"function_call\"]\n",
        "print( \"JSON data : \",json_data)"
      ],
      "metadata": {
        "id": "Esp12nbcRBK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp.llama_chat_format import MoondreamChatHandler\n",
        "\n",
        "chat_handler = MoondreamChatHandler.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*mmproj*\",\n",
        ")\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "  repo_id=\"vikhyatk/moondream2\",\n",
        "  filename=\"*text-model*\",\n",
        "  chat_handler=chat_handler,\n",
        "  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n",
        ")\n",
        "\n",
        "image_output = llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\" : \"text\", \"text\": \"Qu'y a t-il dans cette image?\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n",
        "\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(\"############ image_output ############\")\n",
        "# print(image_output)\n",
        "# reponse = output['choices'][0][\"message\"]\n",
        "print(\"\\nRéponse : \", image_output)\n",
        "# json_data = reponse[\"function_call\"]\n",
        "# print( \"JSON data : \",json_data)"
      ],
      "metadata": {
        "id": "xCxRJa_RYo4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}