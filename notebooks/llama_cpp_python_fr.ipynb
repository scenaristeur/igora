{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfSzgxKmd4FJA8p2oOiYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scenaristeur/igora/blob/main/notebooks/llama_cpp_python_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Téléchargement d'un modèle de langage (dolphin) au format .GGUF depuis HuggingFace / TheBloke https://huggingface.co/TheBloke dans le dossier ./models"
      ],
      "metadata": {
        "id": "-1FrZyiUwr5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVEbYUTdwPWq",
        "outputId": "16ef8b73-16f9-421c-c2b6-eb7d8644d7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 09:58:33--  https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.27, 65.8.178.118, 65.8.178.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1714643913&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDY0MzkxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=agSZxVxMGSI4YW467pLmbzhIZ7Y83b9b8mFgAKOrN756jCUn1qZL%7EF-YzkVeEmerA49r6%7E50NTx7hwJlvhdwIaEc9RBMPmqYGdCzD34cb%7E76sKKYm6c61cuzPvOYfIMqxAqKphGmFSEhWrWImcTLp0igmJ0-4zyDn2XvOoEHnoAtdC4fe6L0qqpvz%7EikWg99FW8T2vgxVmFdUWYrCl2MN-RdeI0syGDxVVK4VCgx4xD9X8RpsGC%7E9ARbZHISO%7EFmJf5MTpQR4HQTzbbIPh14pB-JjTdqELBJYyVf6LKRSRbZlmkF31T03WXpOd58puyXGuepvG-Sm6ZHRI7iLx0YOA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-04-29 09:58:33--  https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1714643913&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDY0MzkxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=agSZxVxMGSI4YW467pLmbzhIZ7Y83b9b8mFgAKOrN756jCUn1qZL%7EF-YzkVeEmerA49r6%7E50NTx7hwJlvhdwIaEc9RBMPmqYGdCzD34cb%7E76sKKYm6c61cuzPvOYfIMqxAqKphGmFSEhWrWImcTLp0igmJ0-4zyDn2XvOoEHnoAtdC4fe6L0qqpvz%7EikWg99FW8T2vgxVmFdUWYrCl2MN-RdeI0syGDxVVK4VCgx4xD9X8RpsGC%7E9ARbZHISO%7EFmJf5MTpQR4HQTzbbIPh14pB-JjTdqELBJYyVf6LKRSRbZlmkF31T03WXpOd58puyXGuepvG-Sm6ZHRI7iLx0YOA__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 13.249.98.99, 13.249.98.111, 13.249.98.125, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|13.249.98.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3083107200 (2.9G) [binary/octet-stream]\n",
            "Saving to: ‘./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf’\n",
            "\n",
            "./models/dolphin-2. 100%[===================>]   2.87G   108MB/s    in 26s     \n",
            "\n",
            "2024-04-29 09:59:00 (111 MB/s) - ‘./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf’ saved [3083107200/3083107200]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir models\n",
        "!wget -O ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation de llama-cpp-python version CPU"
      ],
      "metadata": {
        "id": "DYH7A-JMyV2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4-zT3dyh6-",
        "outputId": "54f52f23-e552-460b-86bd-5e4dae9e6e08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
            "Collecting llama-cpp-python[server]\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.65/llama_cpp_python-0.2.65-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python[server])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (3.1.3)\n",
            "Collecting uvicorn>=0.22.0 (from llama-cpp-python[server])\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.100.0 (from llama-cpp-python[server])\n",
            "  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-settings>=2.0.1 (from llama-cpp-python[server])\n",
            "  Downloading pydantic_settings-2.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from llama-cpp-python[server])\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting starlette-context<0.4,>=0.3.6 (from llama-cpp-python[server])\n",
            "  Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (6.0.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (2.7.0)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (2.1.5)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.1->llama-cpp-python[server])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (2.18.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.2.1)\n",
            "Installing collected packages: python-dotenv, h11, diskcache, uvicorn, starlette, llama-cpp-python, starlette-context, sse-starlette, pydantic-settings, fastapi\n",
            "Successfully installed diskcache-5.6.3 fastapi-0.110.2 h11-0.14.0 llama-cpp-python-0.2.65 pydantic-settings-2.2.1 python-dotenv-1.0.1 sse-starlette-2.1.0 starlette-0.37.2 starlette-context-0.3.6 uvicorn-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On peut maintenant poser une question au modèle de langage en suivant la démo\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "Wc6J5Nl-y8-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Q: (Nomme les planètes du système solaire). A: \", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse\", reponse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLKpbPBYzL-G",
        "outputId": "13054162-0a1f-4db2-871a-2a06b75c0141"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
            "llm_load_print_meta: general.name     = ehartford_dolphin-2.2.1-mistral-7b\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2939.58 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LAMMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'ehartford_dolphin-2.2.1-mistral-7b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =   11603.73 ms\n",
            "llama_print_timings:      sample time =      27.28 ms /    44 runs   (    0.62 ms per token,  1613.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11603.60 ms /    20 tokens (  580.18 ms per token,     1.72 tokens per second)\n",
            "llama_print_timings:        eval time =   33275.14 ms /    43 runs   (  773.84 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:       total time =   45090.78 ms /    63 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-7281f3e6-f550-414e-80bf-47a0aec1bc47', 'object': 'text_completion', 'created': 1714385925, 'model': './models/dolphin-2.2.1-mistral-7b.Q2_K.gguf', 'choices': [{'text': 'Q: (Nomme les planètes du système solaire). A: 1. Mercure 2. Vénus 3. La Terre 4. Mars 5. Jupiter 6. Saturne 7. Uranus 8. Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 44, 'total_tokens': 64}}\n",
            "\n",
            "Réponse Q: (Nomme les planètes du système solaire). A: 1. Mercure 2. Vénus 3. La Terre 4. Mars 5. Jupiter 6. Saturne 7. Uranus 8. Neptune\n"
          ]
        }
      ]
    }
  ]
}