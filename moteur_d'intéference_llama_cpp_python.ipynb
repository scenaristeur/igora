{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOckwg/38ypDOUzEfZ3du6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scenaristeur/igora/blob/main/moteur_d'int%C3%A9ference_llama_cpp_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation du moteur d'inférence llama-cpp-python version CPU\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "YoXOepsZu0nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python[server]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N-jTYOwvJXJ",
        "outputId": "9f63a119-c721-436d-a9de-7732ba185b79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python[server]\n",
            "  Downloading llama_cpp_python-0.2.77.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (4.12.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python[server])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (3.1.4)\n",
            "Collecting uvicorn>=0.22.0 (from llama-cpp-python[server])\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.100.0 (from llama-cpp-python[server])\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-settings>=2.0.1 (from llama-cpp-python[server])\n",
            "  Downloading pydantic_settings-2.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from llama-cpp-python[server])\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting starlette-context<0.4,>=0.3.6 (from llama-cpp-python[server])\n",
            "  Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (6.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (2.7.3)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart>=0.0.7 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (2.1.5)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.1->llama-cpp-python[server])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi>=0.100.0->llama-cpp-python[server]) (3.7)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server]) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.100.0->llama-cpp-python[server]) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (2.18.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn>=0.22.0->llama-cpp-python[server])\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.100.0->llama-cpp-python[server]) (0.1.2)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl size=3726647 sha256=3709f2105babf53dbbb4a2457efb94fbba38c3daec47671b93e67fede14f2f47\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/55/a1/6d6c2ef6fed3ef054b4170d8bcd05a09e6dc971db7fad955ff\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, diskcache, watchfiles, uvicorn, starlette, llama-cpp-python, httpcore, email_validator, typer, starlette-context, sse-starlette, pydantic-settings, httpx, fastapi-cli, fastapi\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llama-cpp-python-0.2.77 orjson-3.10.3 pydantic-settings-2.3.1 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 sse-starlette-2.1.0 starlette-0.37.2 starlette-context-0.3.6 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Téléchargement d'un modèle de langage\n",
        "on va utiliser le modèle \"dolphin\" au format en le téléchargeant sur https://huggingface.co/TheBloke dans le dossier ./models"
      ],
      "metadata": {
        "id": "tZCBx94CvZw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models\n",
        "!wget -O ./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true"
      ],
      "metadata": {
        "id": "289q92EFvhxY",
        "outputId": "b16fe940-f7c2-46a5-ed72-9986a94cfdd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-06 08:52:29--  https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q2_K.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.69.31, 18.239.69.83, 18.239.69.50, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.69.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1717923149&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzkyMzE0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fvxBlg8PWFQocTtIwk3zu28RLUetvb7zPLkKUysFdMxyM4XLAbEf-YnUirtL80zSbcJAjC0Z6g9FpCiQchXG%7E-p9qLCQQatALHkJR73vC8e6a1FaxnhQ-5G0XJI8MltwVdSyEkTM2GvZa0cHQuWm4P108CuAdouwrmUf9o8RdrNwubE3C7cGqjiT%7E63qA11wHahMxsyfj2Mp1H-sxa%7Ee%7ERM5aUAVmQLMsTdC6aZOKgfwmuRu5EOLuhLhCSA9PQx6Ya-Fq7yHNVAiKxRQEDnukoyhoZnywaFJ5Isb1V1qGi7MGTbWQCInj4UihJW3MFKGnpatPB%7ECvzYJFQJZreI43A__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-06-06 08:52:30--  https://cdn-lfs-us-1.huggingface.co/repos/65/43/6543978f14bae789a881f094106b3eeecaafa371896c90db53838cc126b7ef7a/891ab42dad4045dbad1eb6ca5e8f0bc15fe8c6bc4befda6decfe076cb2e5811b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27dolphin-2.2.1-mistral-7b.Q2_K.gguf%3B+filename%3D%22dolphin-2.2.1-mistral-7b.Q2_K.gguf%22%3B&Expires=1717923149&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzkyMzE0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY1LzQzLzY1NDM5NzhmMTRiYWU3ODlhODgxZjA5NDEwNmIzZWVlY2FhZmEzNzE4OTZjOTBkYjUzODM4Y2MxMjZiN2VmN2EvODkxYWI0MmRhZDQwNDVkYmFkMWViNmNhNWU4ZjBiYzE1ZmU4YzZiYzRiZWZkYTZkZWNmZTA3NmNiMmU1ODExYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fvxBlg8PWFQocTtIwk3zu28RLUetvb7zPLkKUysFdMxyM4XLAbEf-YnUirtL80zSbcJAjC0Z6g9FpCiQchXG%7E-p9qLCQQatALHkJR73vC8e6a1FaxnhQ-5G0XJI8MltwVdSyEkTM2GvZa0cHQuWm4P108CuAdouwrmUf9o8RdrNwubE3C7cGqjiT%7E63qA11wHahMxsyfj2Mp1H-sxa%7Ee%7ERM5aUAVmQLMsTdC6aZOKgfwmuRu5EOLuhLhCSA9PQx6Ya-Fq7yHNVAiKxRQEDnukoyhoZnywaFJ5Isb1V1qGi7MGTbWQCInj4UihJW3MFKGnpatPB%7ECvzYJFQJZreI43A__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.4.72, 108.157.4.62, 108.157.4.58, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.4.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3083107200 (2.9G) [binary/octet-stream]\n",
            "Saving to: ‘./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf’\n",
            "\n",
            ".1-mistral-7b.Q2_K.  48%[========>           ]   1.39G  2.73MB/s    eta 1m 57s "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On peut maintenant poser une question au modèle de langage en suivant la démo\n"
      ],
      "metadata": {
        "id": "4ir604JFvtJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "      model_path=\"./models/dolphin-2.2.1-mistral-7b.Q2_K.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Nomme les planètes du système solaire.\", # Prompt\n",
        "      max_tokens=150, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=False, # Echo the prompt back in the output\n",
        "      stream=True\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "reponse = output['choices'][0][\"text\"]\n",
        "print(\"\\nRéponse\", reponse)"
      ],
      "metadata": {
        "id": "Gzl91nfrv2kq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}